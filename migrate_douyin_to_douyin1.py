#!/usr/bin/env python3
"""
Douyinåˆ°Douyin1æ•°æ®è¿ç§»è„šæœ¬

åŠŸèƒ½ï¼š
1. è¿ç§»è®¢é˜…æ•°æ® (subscriptions.json)
2. è¿ç§»å·²çŸ¥å†…å®¹ID (known_item_ids.json)
3. è¿ç§»æ¶ˆæ¯æ˜ å°„ (message_mappings.json)
4. å»ºç«‹URLåˆ°ç›®å½•çš„æ˜ å°„å…³ç³»
5. éªŒè¯è¿ç§»ç»“æœ

ä½œè€…: Assistant
åˆ›å»ºæ—¶é—´: 2024å¹´
"""

import os
import json
import hashlib
import shutil
import logging
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime


class DouyinMigrator:
    """Douyinæ•°æ®è¿ç§»å™¨"""
    
    def __init__(self, dry_run: bool = True):
        """
        åˆå§‹åŒ–è¿ç§»å™¨
        
        Args:
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰
        """
        self.dry_run = dry_run
        self.logger = self._setup_logger()
        
        # æºè·¯å¾„ï¼ˆdouyinï¼‰
        self.source_base = Path("storage/douyin")
        self.source_config = self.source_base / "config"
        self.source_data = self.source_base / "data"
        self.source_media = self.source_base / "media"
        
        # ç›®æ ‡è·¯å¾„ï¼ˆdouyin1ï¼‰
        self.target_base = Path("storage/douyin1")
        self.target_config = self.target_base / "config"
        self.target_data = self.target_base / "data"
        self.target_media = self.target_base / "media"
        
        # è¿ç§»ç»Ÿè®¡
        self.stats = {
            "subscriptions_migrated": 0,
            "known_items_migrated": 0,
            "message_mappings_migrated": 0,
            "directories_created": 0,
            "errors": []
        }
        
        # URLåˆ°ç›®å½•çš„æ˜ å°„å…³ç³»
        self.url_directory_mapping = {}
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("douyin_migrator")
        logger.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        log_file = f"migration_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        
        return logger
    
    def _safe_filename(self, url: str) -> str:
        """
        ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆå¤ç”¨douyin1çš„é€»è¾‘ï¼‰
        
        Args:
            url: URLå­—ç¬¦ä¸²
            
        Returns:
            str: å®‰å…¨çš„æ–‡ä»¶å
        """
        # ç§»é™¤åè®®å‰ç¼€
        clean_url = re.sub(r'^https?://', '', url)
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        clean_url = re.sub(r'[^\w\-_.]', '_', clean_url)
        # é™åˆ¶é•¿åº¦å¹¶æ·»åŠ å“ˆå¸Œ
        if len(clean_url) > 50:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            clean_url = clean_url[:42] + '_' + url_hash
        
        return clean_url
    
    def _get_douyin_hash(self, url: str) -> str:
        """
        ç”Ÿæˆdouyinæ¨¡å—ä½¿ç”¨çš„å“ˆå¸Œå€¼
        
        Args:
            url: URLå­—ç¬¦ä¸²
            
        Returns:
            str: SHA256å“ˆå¸Œå€¼å‰16ä½
        """
        return hashlib.sha256(url.encode()).hexdigest()[:16]
    
    def _ensure_directories(self):
        """ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨"""
        directories = [
            self.target_base,
            self.target_config,
            self.target_data,
            self.target_media
        ]
        
        for directory in directories:
            if not self.dry_run:
                directory.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}åˆ›å»ºç›®å½•: {directory}")
            self.stats["directories_created"] += 1
    
    def _build_url_mapping(self) -> Dict[str, str]:
        """
        æ„å»ºURLåˆ°ç›®å½•çš„æ˜ å°„å…³ç³»
        
        Returns:
            Dict[str, str]: {URL: douyin_hash_dir}
        """
        mapping = {}
        
        if not self.source_data.exists():
            self.logger.warning(f"æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.source_data}")
            return mapping
        
        # éå†æ‰€æœ‰å“ˆå¸Œç›®å½•
        for hash_dir in self.source_data.iterdir():
            if not hash_dir.is_dir():
                continue
                
            url_file = hash_dir / "url.txt"
            if url_file.exists():
                try:
                    url = url_file.read_text(encoding='utf-8').strip()
                    mapping[url] = hash_dir.name
                    self.logger.debug(f"å‘ç°URLæ˜ å°„: {url} -> {hash_dir.name}")
                except Exception as e:
                    self.logger.error(f"è¯»å–URLæ–‡ä»¶å¤±è´¥: {url_file}, é”™è¯¯: {e}")
                    self.stats["errors"].append(f"è¯»å–URLæ–‡ä»¶å¤±è´¥: {url_file}")
        
        self.logger.info(f"æ„å»ºURLæ˜ å°„å®Œæˆï¼Œå…± {len(mapping)} ä¸ªURL")
        return mapping
    
    def migrate_subscriptions(self) -> bool:
        """
        è¿ç§»è®¢é˜…æ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            source_file = self.source_config / "subscriptions.json"
            target_file = self.target_config / "subscriptions.json"
            
            if not source_file.exists():
                self.logger.warning(f"æºè®¢é˜…æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
                return True
            
            # è¯»å–æºæ•°æ®
            with open(source_file, 'r', encoding='utf-8') as f:
                subscriptions = json.load(f)
            
            self.logger.info(f"è¯»å–åˆ° {len(subscriptions)} ä¸ªè®¢é˜…")
            
            # éªŒè¯æ•°æ®æ ¼å¼
            for url, channels in subscriptions.items():
                if not isinstance(channels, list):
                    self.logger.error(f"è®¢é˜…æ•°æ®æ ¼å¼é”™è¯¯: {url} -> {channels}")
                    return False
            
            # å†™å…¥ç›®æ ‡æ–‡ä»¶
            if not self.dry_run:
                with open(target_file, 'w', encoding='utf-8') as f:
                    json.dump(subscriptions, f, ensure_ascii=False, indent=2)
            
            self.stats["subscriptions_migrated"] = len(subscriptions)
            self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}è®¢é˜…æ•°æ®è¿ç§»å®Œæˆ: {len(subscriptions)} ä¸ª")
            return True
            
        except Exception as e:
            self.logger.error(f"è¿ç§»è®¢é˜…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"è¿ç§»è®¢é˜…æ•°æ®å¤±è´¥: {e}")
            return False
    
    def migrate_known_items(self) -> bool:
        """
        è¿ç§»å·²çŸ¥å†…å®¹IDæ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            migrated_count = 0
            
            for url, hash_dir in self.url_directory_mapping.items():
                source_dir = self.source_data / hash_dir
                source_file = source_dir / "known_item_ids.json"
                
                if not source_file.exists():
                    self.logger.debug(f"å·²çŸ¥å†…å®¹æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
                    continue
                
                # è¯»å–æºæ•°æ®
                with open(source_file, 'r', encoding='utf-8') as f:
                    known_items = json.load(f)
                
                # åˆ›å»ºç›®æ ‡ç›®å½•
                safe_name = self._safe_filename(url)
                target_dir = self.target_data / safe_name
                target_file = target_dir / "known_item_ids.json"
                
                if not self.dry_run:
                    target_dir.mkdir(parents=True, exist_ok=True)
                    with open(target_file, 'w', encoding='utf-8') as f:
                        json.dump(known_items, f, ensure_ascii=False, indent=2)
                
                migrated_count += 1
                self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}è¿ç§»å·²çŸ¥å†…å®¹: {url} ({len(known_items)} ä¸ª)")
            
            self.stats["known_items_migrated"] = migrated_count
            self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}å·²çŸ¥å†…å®¹è¿ç§»å®Œæˆ: {migrated_count} ä¸ªURL")
            return True
            
        except Exception as e:
            self.logger.error(f"è¿ç§»å·²çŸ¥å†…å®¹å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"è¿ç§»å·²çŸ¥å†…å®¹å¤±è´¥: {e}")
            return False
    
    def migrate_message_mappings(self) -> bool:
        """
        è¿ç§»æ¶ˆæ¯æ˜ å°„æ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            source_file = self.source_config / "message_mappings.json"
            target_file = self.target_config / "message_mappings.json"
            
            if not source_file.exists():
                self.logger.warning(f"æºæ¶ˆæ¯æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
                # åˆ›å»ºç©ºçš„æ¶ˆæ¯æ˜ å°„æ–‡ä»¶
                if not self.dry_run:
                    with open(target_file, 'w', encoding='utf-8') as f:
                        json.dump({}, f, ensure_ascii=False, indent=2)
                return True
            
            # è¯»å–æºæ•°æ®
            with open(source_file, 'r', encoding='utf-8') as f:
                message_mappings = json.load(f)
            
            self.logger.info(f"è¯»å–åˆ° {len(message_mappings)} ä¸ªæ¶ˆæ¯æ˜ å°„")
            
            # å†™å…¥ç›®æ ‡æ–‡ä»¶
            if not self.dry_run:
                with open(target_file, 'w', encoding='utf-8') as f:
                    json.dump(message_mappings, f, ensure_ascii=False, indent=2)
            
            self.stats["message_mappings_migrated"] = len(message_mappings)
            self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}æ¶ˆæ¯æ˜ å°„è¿ç§»å®Œæˆ: {len(message_mappings)} ä¸ª")
            return True
            
        except Exception as e:
            self.logger.error(f"è¿ç§»æ¶ˆæ¯æ˜ å°„å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"è¿ç§»æ¶ˆæ¯æ˜ å°„å¤±è´¥: {e}")
            return False
    
    def verify_migration(self) -> bool:
        """
        éªŒè¯è¿ç§»ç»“æœ
        
        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            self.logger.info("å¼€å§‹éªŒè¯è¿ç§»ç»“æœ...")
            
            # éªŒè¯è®¢é˜…æ•°æ®
            source_subs_file = self.source_config / "subscriptions.json"
            target_subs_file = self.target_config / "subscriptions.json"
            
            if source_subs_file.exists() and target_subs_file.exists():
                with open(source_subs_file, 'r', encoding='utf-8') as f:
                    source_subs = json.load(f)
                with open(target_subs_file, 'r', encoding='utf-8') as f:
                    target_subs = json.load(f)
                
                if source_subs == target_subs:
                    self.logger.info("âœ… è®¢é˜…æ•°æ®éªŒè¯é€šè¿‡")
                else:
                    self.logger.error("âŒ è®¢é˜…æ•°æ®éªŒè¯å¤±è´¥")
                    return False
            
            # éªŒè¯å·²çŸ¥å†…å®¹æ•°æ®
            verified_count = 0
            for url, hash_dir in self.url_directory_mapping.items():
                source_file = self.source_data / hash_dir / "known_item_ids.json"
                if not source_file.exists():
                    continue
                
                safe_name = self._safe_filename(url)
                target_file = self.target_data / safe_name / "known_item_ids.json"
                
                if target_file.exists():
                    with open(source_file, 'r', encoding='utf-8') as f:
                        source_items = json.load(f)
                    with open(target_file, 'r', encoding='utf-8') as f:
                        target_items = json.load(f)
                    
                    if source_items == target_items:
                        verified_count += 1
                    else:
                        self.logger.error(f"âŒ å·²çŸ¥å†…å®¹æ•°æ®éªŒè¯å¤±è´¥: {url}")
                        return False
            
            self.logger.info(f"âœ… å·²çŸ¥å†…å®¹æ•°æ®éªŒè¯é€šè¿‡: {verified_count} ä¸ªURL")
            
            # éªŒè¯æ¶ˆæ¯æ˜ å°„
            source_msg_file = self.source_config / "message_mappings.json"
            target_msg_file = self.target_config / "message_mappings.json"
            
            if source_msg_file.exists() and target_msg_file.exists():
                with open(source_msg_file, 'r', encoding='utf-8') as f:
                    source_msgs = json.load(f)
                with open(target_msg_file, 'r', encoding='utf-8') as f:
                    target_msgs = json.load(f)
                
                if source_msgs == target_msgs:
                    self.logger.info("âœ… æ¶ˆæ¯æ˜ å°„éªŒè¯é€šè¿‡")
                else:
                    self.logger.error("âŒ æ¶ˆæ¯æ˜ å°„éªŒè¯å¤±è´¥")
                    return False
            
            self.logger.info("âœ… æ‰€æœ‰æ•°æ®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            self.logger.error(f"éªŒè¯è¿ç§»ç»“æœå¤±è´¥: {e}", exc_info=True)
            return False
    
    def create_backup(self) -> bool:
        """
        åˆ›å»ºæºæ•°æ®å¤‡ä»½
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            if not self.source_base.exists():
                self.logger.warning(f"æºç›®å½•ä¸å­˜åœ¨: {self.source_base}")
                return True
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_dir = Path(f"storage/douyin_backup_{timestamp}")
            
            if not self.dry_run:
                shutil.copytree(self.source_base, backup_dir)
            
            self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}åˆ›å»ºå¤‡ä»½: {backup_dir}")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def run_migration(self, create_backup: bool = True) -> bool:
        """
        æ‰§è¡Œå®Œæ•´è¿ç§»æµç¨‹
        
        Args:
            create_backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        self.logger.info(f"å¼€å§‹æ•°æ®è¿ç§» {'(è¯•è¿è¡Œæ¨¡å¼)' if self.dry_run else '(å®é™…æ‰§è¡Œ)'}")
        
        try:
            # 1. åˆ›å»ºå¤‡ä»½
            if create_backup:
                if not self.create_backup():
                    return False
            
            # 2. ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            self._ensure_directories()
            
            # 3. æ„å»ºURLæ˜ å°„
            self.url_directory_mapping = self._build_url_mapping()
            
            # 4. è¿ç§»è®¢é˜…æ•°æ®
            if not self.migrate_subscriptions():
                return False
            
            # 5. è¿ç§»å·²çŸ¥å†…å®¹
            if not self.migrate_known_items():
                return False
            
            # 6. è¿ç§»æ¶ˆæ¯æ˜ å°„
            if not self.migrate_message_mappings():
                return False
            
            # 7. éªŒè¯è¿ç§»ç»“æœï¼ˆä»…åœ¨å®é™…æ‰§è¡Œæ—¶ï¼‰
            if not self.dry_run:
                if not self.verify_migration():
                    return False
            
            # 8. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_statistics()
            
            self.logger.info("ğŸ‰ æ•°æ®è¿ç§»å®Œæˆï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return False
    
    def print_statistics(self):
        """æ‰“å°è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("=" * 50)
        self.logger.info("ğŸ“Š è¿ç§»ç»Ÿè®¡ä¿¡æ¯")
        self.logger.info("=" * 50)
        self.logger.info(f"è®¢é˜…æ•°æ®è¿ç§»: {self.stats['subscriptions_migrated']} ä¸ª")
        self.logger.info(f"å·²çŸ¥å†…å®¹è¿ç§»: {self.stats['known_items_migrated']} ä¸ªURL")
        self.logger.info(f"æ¶ˆæ¯æ˜ å°„è¿ç§»: {self.stats['message_mappings_migrated']} ä¸ª")
        self.logger.info(f"åˆ›å»ºç›®å½•æ•°é‡: {self.stats['directories_created']} ä¸ª")
        self.logger.info(f"é”™è¯¯æ•°é‡: {len(self.stats['errors'])} ä¸ª")
        
        if self.stats['errors']:
            self.logger.info("\nâŒ é”™è¯¯è¯¦æƒ…:")
            for error in self.stats['errors']:
                self.logger.info(f"  - {error}")
        
        self.logger.info("=" * 50)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Douyinåˆ°Douyin1æ•°æ®è¿ç§»å·¥å…·')
    parser.add_argument('--dry-run', action='store_true', 
                       help='è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶')
    parser.add_argument('--no-backup', action='store_true',
                       help='ä¸åˆ›å»ºå¤‡ä»½')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆ›å»ºè¿ç§»å™¨
    migrator = DouyinMigrator(dry_run=args.dry_run)
    
    # æ‰§è¡Œè¿ç§»
    success = migrator.run_migration(create_backup=not args.no_backup)
    
    if success:
        print("âœ… è¿ç§»å®Œæˆï¼")
        if args.dry_run:
            print("ğŸ’¡ è¿™æ˜¯è¯•è¿è¡Œæ¨¡å¼ï¼Œè¦å®é™…æ‰§è¡Œè¯·ç§»é™¤ --dry-run å‚æ•°")
    else:
        print("âŒ è¿ç§»å¤±è´¥ï¼è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")
        exit(1)


if __name__ == "__main__":
    main() 