"""
Sitemapè§£æå™¨æ¨¡å—

æä¾›Sitemapè§£æåŠŸèƒ½ï¼Œæ”¯æŒXMLå’ŒTXTæ ¼å¼ï¼Œä»¥åŠå‹ç¼©æ ¼å¼ã€‚
æ”¯æŒè§£æSitemapç´¢å¼•æ–‡ä»¶ã€‚

ä½œè€…: Assistant
åˆ›å»ºæ—¶é—´: 2024å¹´
"""

import logging
import gzip
from typing import List, Optional, Union, Dict, Any
from datetime import datetime
import requests
from urllib.parse import urlparse
import hashlib
import lxml.etree as etree

from .sitemap_entry import SitemapEntry
from services.common.cache import get_cache

logger = logging.getLogger(__name__)

class SitemapParser:
    """Sitemapè§£æå™¨"""

    def __init__(self, timeout: int = 30, max_retries: int = 3, cache_ttl: int = 21600):
        """
        åˆå§‹åŒ–è§£æå™¨

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤6å°æ—¶
        """
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆå§‹åŒ–ç¼“å­˜
        self.cache = get_cache("sitemap_parser", ttl=cache_ttl)

        logger.info(f"Sitemapè§£æå™¨åˆå§‹åŒ–å®Œæˆï¼Œè¶…æ—¶: {timeout}s, é‡è¯•: {max_retries}æ¬¡, ç¼“å­˜TTL: {cache_ttl}s")

    def _generate_cache_key(self, url: str) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            url: Sitemap URL

        Returns:
            str: ç¼“å­˜é”®
        """
        # ä½¿ç”¨URLç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®
        cache_key = hashlib.md5(url.encode('utf-8')).hexdigest()
        return f"sitemap_feed:{cache_key}"

    def parse(self, url: str) -> List[SitemapEntry]:
        """
        è§£æSitemap

        Args:
            url: Sitemap URL

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨

        Raises:
            ValueError: URLæ ¼å¼é”™è¯¯
            requests.RequestException: ç½‘ç»œè¯·æ±‚é”™è¯¯
            etree.ParseError: XMLè§£æé”™è¯¯
        """
        # éªŒè¯URL
        if not url.startswith(('http://', 'https://')):
            raise ValueError(f"Invalid URL: {url}")

        try:
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(url)

            # å°è¯•ä»ç¼“å­˜è·å–æ•°æ®
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                logger.info(f"ğŸ“¦ ä»ç¼“å­˜è·å–Sitemapå†…å®¹: {url}, æ¡ç›®æ•°: {len(cached_data)}")
                # å°†ç¼“å­˜çš„å­—å…¸æ•°æ®è½¬æ¢å›SitemapEntryå¯¹è±¡
                entries = []
                for entry_dict in cached_data:
                    try:
                        entry = self._dict_to_sitemap_entry(entry_dict)
                        if entry:
                            entries.append(entry)
                    except Exception as e:
                        logger.warning(f"ç¼“å­˜æ¡ç›®è½¬æ¢å¤±è´¥: {str(e)}")
                        continue
                return entries

            logger.info(f"ğŸŒ å¼€å§‹è§£æSitemap: {url}")

            # è·å–å†…å®¹ï¼ˆåŒæ­¥requestså®ç°ï¼‰
            response = requests.get(url, timeout=self.timeout)
            response.raise_for_status()
            content = response.content

            # æ£€æŸ¥æ˜¯å¦æ˜¯gzipå‹ç¼©
            is_gzip = False
            if url.endswith('.gz'):
                is_gzip = True
            elif response.headers.get('content-encoding') == 'gzip':
                is_gzip = True
            elif content.startswith(b'\x1f\x8b'):  # gzip é­”æ•°
                is_gzip = True

            if is_gzip:
                try:
                    content = gzip.decompress(content)
                except Exception as e:
                    logger.warning(f"gzipè§£å‹å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸå§‹å†…å®¹: {str(e)}")

            content = content.decode('utf-8')

            # æ ¹æ®URLåç¼€åˆ¤æ–­æ ¼å¼
            if url.endswith('.txt'):
                entries = self._parse_txt(content)
            else:
                entries = self._parse_xml(content, url)

            # ç¼“å­˜è§£æç»“æœï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
            if entries:
                cache_data = []
                for entry in entries:
                    try:
                        entry_dict = self._sitemap_entry_to_dict(entry)
                        cache_data.append(entry_dict)
                    except Exception as e:
                        logger.warning(f"æ¡ç›®åºåˆ—åŒ–å¤±è´¥: {str(e)}")
                        continue

                self.cache.set(cache_key, cache_data)
                logger.info(f"ğŸ’¾ Sitemapå†…å®¹å·²ç¼“å­˜: {url}, æ¡ç›®æ•°: {len(cache_data)}")

            return entries

        except Exception as e:
            logger.error(f"è§£æSitemapå¤±è´¥: {url}, é”™è¯¯: {str(e)}", exc_info=True)
            raise

    def _parse_txt(self, content: str) -> List[SitemapEntry]:
        """
        è§£æTXTæ ¼å¼

        Args:
            content: æ–‡ä»¶å†…å®¹

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨
        """
        entries = []
        for line in content.splitlines():
            line = line.strip()
            if line and line.startswith(('http://', 'https://')):
                entries.append(SitemapEntry(url=line))
        return entries

    def _parse_xml(self, content: str, base_url: str) -> List[SitemapEntry]:
        """
        è§£æXMLæ ¼å¼

        Args:
            content: XMLå†…å®¹
            base_url: åŸºç¡€URLï¼Œç”¨äºè§£æç›¸å¯¹è·¯å¾„

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨

        Raises:
            etree.ParseError: XMLè§£æé”™è¯¯
        """
        try:
            parser = etree.XMLParser(remove_blank_text=True)
            root = etree.fromstring(content.encode('utf-8'), parser=parser)

            # æ£€æŸ¥æ˜¯å¦æ˜¯sitemapç´¢å¼•
            if root.tag.endswith('sitemapindex'):
                return self._parse_sitemap_index(root, base_url)

            # è§£ææ™®é€šsitemap
            entries = []
            logger.info("å¼€å§‹è§£ææ™®é€šsitemap")

            # è·å–æ‰€æœ‰å‘½åç©ºé—´
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9',
                'xhtml': 'http://www.w3.org/1999/xhtml',
                'image': 'http://www.google.com/schemas/sitemap-image/1.1',
                'video': 'http://www.google.com/schemas/sitemap-video/1.1',
                'news': 'http://www.google.com/schemas/sitemap-news/0.9'
            }
            logger.debug(f"ä½¿ç”¨çš„å‘½åç©ºé—´: {namespaces}")

            # ä½¿ç”¨XPathæŸ¥æ‰¾æ‰€æœ‰URLå…ƒç´ 
            url_elements = root.xpath('.//ns:url', namespaces=namespaces)
            logger.info(f"æ‰¾åˆ° {len(url_elements)} ä¸ªURLå…ƒç´ ")

            # ç»Ÿè®¡ä¿¡æ¯
            total_urls = len(url_elements)
            valid_urls = 0
            invalid_urls = 0
            time_parse_errors = 0

            for i, url in enumerate(url_elements, 1):
                try:
                    logger.debug(f"å¤„ç†ç¬¬ {i}/{total_urls} ä¸ªURLå…ƒç´ ")

                    # è·å–URL
                    loc = url.find('ns:loc', namespaces=namespaces)
                    if loc is None or not loc.text:
                        logger.warning(f"URLå…ƒç´  {i} ç¼ºå°‘locæ ‡ç­¾æˆ–å†…å®¹ä¸ºç©º")
                        invalid_urls += 1
                        continue

                    url_text = loc.text.strip()
                    logger.debug(f"URL {i}: {url_text}")

                    # è·å–æœ€åä¿®æ”¹æ—¶é—´
                    last_modified = None
                    lastmod = url.find('ns:lastmod', namespaces=namespaces)
                    if lastmod is not None and lastmod.text:
                        try:
                            # å¤„ç†ISO 8601æ ¼å¼çš„æ—¶é—´
                            time_str = lastmod.text.strip()
                            logger.debug(f"åŸå§‹æ—¶é—´å­—ç¬¦ä¸²: {time_str}")

                            if time_str.endswith('Z'):
                                time_str = time_str[:-1] + '+00:00'
                                logger.debug(f"è½¬æ¢åçš„æ—¶é—´å­—ç¬¦ä¸²: {time_str}")

                            last_modified = datetime.fromisoformat(time_str)
                            logger.debug(f"è§£ææ—¶é—´æˆåŠŸ: {time_str} -> {last_modified}")
                        except ValueError as e:
                            logger.warning(f"è§£ææ—¶é—´å¤±è´¥: {lastmod.text}, é”™è¯¯: {str(e)}", exc_info=True)
                            time_parse_errors += 1
                    else:
                        logger.debug(f"URL {i} æ²¡æœ‰æœ€åä¿®æ”¹æ—¶é—´")

                    # åˆ›å»ºSitemapEntry
                    entry = SitemapEntry(
                        url=url_text,
                        last_modified=last_modified
                    )
                    entries.append(entry)
                    valid_urls += 1

                except Exception as e:
                    logger.warning(f"å¤„ç†URLå…ƒç´  {i} å¤±è´¥: {str(e)}", exc_info=True)
                    invalid_urls += 1
                    continue

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"è§£æå®Œæˆ - æ€»æ•°: {total_urls}, æœ‰æ•ˆ: {valid_urls}, æ— æ•ˆ: {invalid_urls}, æ—¶é—´è§£æé”™è¯¯: {time_parse_errors}")

            return entries

        except Exception as e:
            logger.error(f"è§£æXMLå¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _parse_sitemap_index(self, root: etree._Element, base_url: str) -> List[SitemapEntry]:
        """
        è§£æSitemapç´¢å¼•

        Args:
            root: XMLæ ¹å…ƒç´ 
            base_url: åŸºç¡€URL

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨
        """
        try:
            all_entries = []
            namespaces = {
                'ns': root.nsmap.get(None, 'http://www.sitemaps.org/schemas/sitemap/0.9')
            }

            # è·å–æ‰€æœ‰sitemap URL
            sitemap_elements = root.xpath('.//ns:sitemap', namespaces=namespaces)
            logger.info(f"æ‰¾åˆ° {len(sitemap_elements)} ä¸ªsitemapç´¢å¼•")

            for sitemap in sitemap_elements:
                try:
                    # è·å–sitemap URL
                    loc = sitemap.find('ns:loc', namespaces=namespaces)
                    if loc is None or not loc.text:
                        continue

                    url = loc.text.strip()
                    logger.info(f"å¤„ç†sitemap: {url}")

                    try:
                        # é€’å½’è§£ææ¯ä¸ªsitemap
                        entries = self.parse(url)
                        all_entries.extend(entries)
                    except Exception as e:
                        logger.warning(f"è§£æsitemapå¤±è´¥: {url}, é”™è¯¯: {str(e)}", exc_info=True)
                        continue

                except Exception as e:
                    logger.warning(f"å¤„ç†sitemapå…ƒç´ å¤±è´¥: {str(e)}", exc_info=True)
                    continue

            return all_entries

        except Exception as e:
            logger.error(f"è§£æsitemapç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _sitemap_entry_to_dict(self, entry: SitemapEntry) -> Dict[str, Any]:
        """
        å°†SitemapEntryè½¬æ¢ä¸ºå­—å…¸

        Args:
            entry: SitemapEntryå¯¹è±¡

        Returns:
            Dict[str, Any]: å­—å…¸æ ¼å¼çš„æ•°æ®
        """
        return {
            'url': entry.url,
            'last_modified': entry.last_modified.isoformat() if entry.last_modified else None
        }

    def _dict_to_sitemap_entry(self, entry_dict: Dict[str, Any]) -> Optional[SitemapEntry]:
        """
        å°†å­—å…¸è½¬æ¢ä¸ºSitemapEntry

        Args:
            entry_dict: å­—å…¸æ ¼å¼çš„æ•°æ®

        Returns:
            Optional[SitemapEntry]: SitemapEntryå¯¹è±¡ï¼Œè½¬æ¢å¤±è´¥è¿”å›None
        """
        try:
            url = entry_dict.get('url')
            if not url:
                return None

            last_modified = None
            if entry_dict.get('last_modified'):
                try:
                    last_modified = datetime.fromisoformat(entry_dict['last_modified'])
                except ValueError:
                    pass

            return SitemapEntry(
                url=url,
                last_modified=last_modified
            )
        except Exception as e:
            logger.warning(f"è½¬æ¢å­—å…¸åˆ°SitemapEntryå¤±è´¥: {str(e)}", exc_info=True)
            return None

    def clear_cache(self, url: str = None) -> bool:
        """
        æ¸…é™¤ç¼“å­˜

        Args:
            url: è¦æ¸…é™¤çš„URLç¼“å­˜ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            if url:
                cache_key = self._generate_cache_key(url)
                self.cache.delete(cache_key)
                logger.info(f"æ¸…é™¤ç¼“å­˜æˆåŠŸ: {url}")
            else:
                self.cache.clear()
                logger.info("æ¸…é™¤æ‰€æœ‰ç¼“å­˜æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}", exc_info=True)
            return False

    def get_cache_info(self) -> Dict:
        """
        è·å–ç¼“å­˜ä¿¡æ¯

        Returns:
            Dict: ç¼“å­˜ä¿¡æ¯
        """
        try:
            return {
                'size': self.cache.size(),
                'ttl': self.cache.ttl,
                'hits': self.cache.hits,
                'misses': self.cache.misses
            }
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
            return {}

    def is_cache_hit(self, url: str) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦å‘½ä¸­ç¼“å­˜

        Args:
            url: è¦æ£€æŸ¥çš„URL

        Returns:
            bool: æ˜¯å¦å‘½ä¸­ç¼“å­˜
        """
        try:
            cache_key = self._generate_cache_key(url)
            return self.cache.exists(cache_key)
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç¼“å­˜å‘½ä¸­å¤±è´¥: {str(e)}", exc_info=True)
            return False

def create_sitemap_parser(timeout: int = 30, max_retries: int = 3, cache_ttl: int = 21600) -> SitemapParser:
    """
    åˆ›å»ºSitemapè§£æå™¨å®ä¾‹

    Args:
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        SitemapParser: è§£æå™¨å®ä¾‹
    """
    return SitemapParser(timeout=timeout, max_retries=max_retries, cache_ttl=cache_ttl)