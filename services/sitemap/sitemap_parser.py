"""
Sitemapè§£æå™¨æ¨¡å—

æä¾›Sitemapè§£æåŠŸèƒ½ï¼Œæ”¯æŒXMLå’ŒTXTæ ¼å¼ï¼Œä»¥åŠå‹ç¼©æ ¼å¼ã€‚
æ”¯æŒè§£æSitemapç´¢å¼•æ–‡ä»¶ã€‚

ä½œè€…: Assistant
åˆ›å»ºæ—¶é—´: 2024å¹´
"""

import logging
import gzip
from typing import List, Optional, Union, Dict, Any
from datetime import datetime
import requests
from urllib.parse import urlparse
import hashlib
import lxml.etree as etree
from dotenv import load_dotenv

from .sitemap_entry import SitemapEntry
from services.common.cache import get_cache

logger = logging.getLogger(__name__)

class SitemapParser:
    """Sitemapè§£æå™¨"""

    def __init__(self, timeout: int = 30, max_retries: int = 3, cache_ttl: int = 21600):
        """
        åˆå§‹åŒ–è§£æå™¨

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤6å°æ—¶
        """
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆå§‹åŒ–ç¼“å­˜
        self.cache = get_cache("sitemap_parser", ttl=cache_ttl)

        logger.info(f"Sitemapè§£æå™¨åˆå§‹åŒ–å®Œæˆï¼Œè¶…æ—¶: {timeout}s, é‡è¯•: {max_retries}æ¬¡, ç¼“å­˜TTL: {cache_ttl}s")

    def _generate_cache_key(self, url: str) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            url: Sitemap URL

        Returns:
            str: ç¼“å­˜é”®
        """
        # ä½¿ç”¨URLç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®
        cache_key = hashlib.md5(url.encode('utf-8')).hexdigest()
        return f"sitemap_feed:{cache_key}"

    def parse(self, url: str) -> List[SitemapEntry]:
        """
        è§£æSitemap

        Args:
            url: Sitemap URL

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨

        Raises:
            requests.RequestException: ç½‘ç»œè¯·æ±‚é”™è¯¯
            etree.ParseError: XMLè§£æé”™è¯¯
        """
        try:
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(url)

            # å°è¯•ä»ç¼“å­˜è·å–æ•°æ®
            cached_content = self.cache.get(cache_key)
            if cached_content is not None:
                logger.info(f"ğŸ“¦ ä»ç¼“å­˜è·å–Sitemapå†…å®¹: {url}")
                content = cached_content
            else:
                logger.info(f"ğŸŒ å¼€å§‹è§£æSitemap: {url}")

                # è·å–å†…å®¹ï¼ˆåŒæ­¥requestså®ç°ï¼‰
                response = requests.get(url, timeout=self.timeout)
                response.raise_for_status()
                content = response.content

                # ç¼“å­˜åŸå§‹å†…å®¹
                self.cache.set(cache_key, content)
                logger.info(f"ğŸ’¾ Sitemapå†…å®¹å·²ç¼“å­˜: {url}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯gzipå‹ç¼©
            is_gzip = False
            if url.endswith('.gz'):
                is_gzip = True
            elif isinstance(content, bytes) and content.startswith(b'\x1f\x8b'):  # gzip é­”æ•°
                is_gzip = True

            if is_gzip:
                try:
                    content = gzip.decompress(content)
                except Exception as e:
                    logger.warning(f"gzipè§£å‹å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸå§‹å†…å®¹: {str(e)}")

            content = content.decode('utf-8')

            # æ ¹æ®URLåç¼€åˆ¤æ–­æ ¼å¼
            if url.endswith('.txt'):
                entries = self._parse_txt(content)
            else:
                entries = self._parse_xml(content, url)

            # é™åˆ¶æ•°é‡
            entries = entries[:10]
            logger.info(f"ğŸ“¦ é™åˆ¶å¤„ç†æœ€è¿‘çš„{len(entries)}ä¸ªæ¡ç›®")

            return entries

        except Exception as e:
            logger.error(f"è§£æSitemapå¤±è´¥: {url}, é”™è¯¯: {str(e)}", exc_info=True)
            raise

    def _parse_txt(self, content: str) -> List[SitemapEntry]:
        """
        è§£æTXTæ ¼å¼

        Args:
            content: æ–‡ä»¶å†…å®¹

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨
        """
        entries = []
        for line in content.splitlines():
            line = line.strip()
            if line and line.startswith(('http://', 'https://')):
                entries.append(SitemapEntry(url=line))
        return entries

    def _parse_xml(self, content: str, base_url: str) -> List[SitemapEntry]:
        """
        è§£æXMLæ ¼å¼

        Args:
            content: XMLå†…å®¹
            base_url: åŸºç¡€URLï¼Œç”¨äºè§£æç›¸å¯¹è·¯å¾„

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨

        Raises:
            etree.ParseError: XMLè§£æé”™è¯¯
        """
        try:
            parser = etree.XMLParser(remove_blank_text=True)
            root = etree.fromstring(content.encode('utf-8'), parser=parser)

            # æ£€æŸ¥æ˜¯å¦æ˜¯sitemapç´¢å¼•
            if root.tag.endswith('sitemapindex'):
                return self._parse_sitemap_index(root, base_url)

            # è§£ææ™®é€šsitemap
            entries = []
            logger.info("å¼€å§‹è§£ææ™®é€šsitemap")

            # è·å–æ‰€æœ‰å‘½åç©ºé—´
            namespaces = {
                'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9',
                'xhtml': 'http://www.w3.org/1999/xhtml',
                'image': 'http://www.google.com/schemas/sitemap-image/1.1',
                'video': 'http://www.google.com/schemas/sitemap-video/1.1',
                'news': 'http://www.google.com/schemas/sitemap-news/0.9'
            }
            logger.debug(f"ä½¿ç”¨çš„å‘½åç©ºé—´: {namespaces}")

            # ä½¿ç”¨XPathæŸ¥æ‰¾æ‰€æœ‰URLå…ƒç´ 
            url_elements = root.xpath('.//ns:url', namespaces=namespaces)
            logger.info(f"æ‰¾åˆ° {len(url_elements)} ä¸ªURLå…ƒç´ ")

            # ç»Ÿè®¡ä¿¡æ¯
            total_urls = len(url_elements)
            valid_urls = 0
            invalid_urls = 0
            time_parse_errors = 0

            for i, url in enumerate(url_elements, 1):
                try:
                    logger.debug(f"å¤„ç†ç¬¬ {i}/{total_urls} ä¸ªURLå…ƒç´ ")

                    # è·å–URL
                    loc = url.find('ns:loc', namespaces=namespaces)
                    if loc is None or not loc.text:
                        logger.warning(f"URLå…ƒç´  {i} ç¼ºå°‘locæ ‡ç­¾æˆ–å†…å®¹ä¸ºç©º")
                        invalid_urls += 1
                        continue

                    url_text = loc.text.strip()
                    logger.debug(f"URL {i}: {url_text}")

                    # è·å–æœ€åä¿®æ”¹æ—¶é—´
                    last_modified = None
                    lastmod = url.find('ns:lastmod', namespaces=namespaces)
                    if lastmod is not None and lastmod.text:
                        try:
                            # å¤„ç†ISO 8601æ ¼å¼çš„æ—¶é—´
                            time_str = lastmod.text.strip()
                            logger.debug(f"åŸå§‹æ—¶é—´å­—ç¬¦ä¸²: {time_str}")

                            if time_str.endswith('Z'):
                                time_str = time_str[:-1] + '+00:00'
                                logger.debug(f"è½¬æ¢åçš„æ—¶é—´å­—ç¬¦ä¸²: {time_str}")

                            last_modified = datetime.fromisoformat(time_str)
                            logger.debug(f"è§£ææ—¶é—´æˆåŠŸ: {time_str} -> {last_modified}")
                        except ValueError as e:
                            logger.warning(f"è§£ææ—¶é—´å¤±è´¥: {lastmod.text}, é”™è¯¯: {str(e)}", exc_info=True)
                            time_parse_errors += 1
                    else:
                        logger.debug(f"URL {i} æ²¡æœ‰æœ€åä¿®æ”¹æ—¶é—´")

                    # åˆ›å»ºSitemapEntry
                    entry = SitemapEntry(
                        url=url_text,
                        last_modified=last_modified
                    )
                    entries.append(entry)
                    valid_urls += 1

                except Exception as e:
                    logger.warning(f"å¤„ç†URLå…ƒç´  {i} å¤±è´¥: {str(e)}", exc_info=True)
                    invalid_urls += 1
                    continue

            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            logger.info(f"è§£æå®Œæˆ - æ€»æ•°: {total_urls}, æœ‰æ•ˆ: {valid_urls}, æ— æ•ˆ: {invalid_urls}, æ—¶é—´è§£æé”™è¯¯: {time_parse_errors}")

            return entries

        except Exception as e:
            logger.error(f"è§£æXMLå¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _parse_sitemap_index(self, root: etree._Element, base_url: str) -> List[SitemapEntry]:
        """
        è§£æSitemapç´¢å¼•

        Args:
            root: XMLæ ¹å…ƒç´ 
            base_url: åŸºç¡€URL

        Returns:
            List[SitemapEntry]: è§£æç»“æœåˆ—è¡¨
        """
        try:
            all_entries = []
            namespaces = {
                'ns': root.nsmap.get(None, 'http://www.sitemaps.org/schemas/sitemap/0.9')
            }

            # è·å–æ‰€æœ‰sitemap URL
            sitemap_elements = root.xpath('.//ns:sitemap', namespaces=namespaces)
            logger.info(f"æ‰¾åˆ° {len(sitemap_elements)} ä¸ªsitemapç´¢å¼•")

            for sitemap in sitemap_elements:
                try:
                    # è·å–sitemap URL
                    loc = sitemap.find('ns:loc', namespaces=namespaces)
                    if loc is None or not loc.text:
                        continue

                    url = loc.text.strip()
                    logger.info(f"å¤„ç†sitemap: {url}")

                    try:
                        # é€’å½’è§£ææ¯ä¸ªsitemap
                        entries = self.parse(url)
                        all_entries.extend(entries)
                    except Exception as e:
                        logger.warning(f"è§£æsitemapå¤±è´¥: {url}, é”™è¯¯: {str(e)}", exc_info=True)
                        continue

                except Exception as e:
                    logger.warning(f"å¤„ç†sitemapå…ƒç´ å¤±è´¥: {str(e)}", exc_info=True)
                    continue

            return all_entries

        except Exception as e:
            logger.error(f"è§£æsitemapç´¢å¼•å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def clear_cache(self, url: str = None) -> bool:
        """
        æ¸…é™¤ç¼“å­˜

        Args:
            url: è¦æ¸…é™¤çš„URLç¼“å­˜ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            if url:
                cache_key = self._generate_cache_key(url)
                self.cache.delete(cache_key)
                logger.info(f"æ¸…é™¤ç¼“å­˜æˆåŠŸ: {url}")
            else:
                self.cache.clear()
                logger.info("æ¸…é™¤æ‰€æœ‰ç¼“å­˜æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}", exc_info=True)
            return False

    def get_cache_info(self) -> Dict:
        """
        è·å–ç¼“å­˜ä¿¡æ¯

        Returns:
            Dict: ç¼“å­˜ä¿¡æ¯
        """
        try:
            return {
                'size': self.cache.size(),
                'ttl': self.cache.ttl,
                'hits': self.cache.hits,
                'misses': self.cache.misses
            }
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
            return {}

    def is_cache_hit(self, url: str) -> bool:
        """
        æ£€æŸ¥URLæ˜¯å¦å‘½ä¸­ç¼“å­˜

        Args:
            url: è¦æ£€æŸ¥çš„URL

        Returns:
            bool: æ˜¯å¦å‘½ä¸­ç¼“å­˜
        """
        try:
            cache_key = self._generate_cache_key(url)
            return self.cache.exists(cache_key)
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç¼“å­˜å‘½ä¸­å¤±è´¥: {str(e)}", exc_info=True)
            return False

def create_sitemap_parser(timeout: int = 30, max_retries: int = 3, cache_ttl: int = 21600) -> SitemapParser:
    """
    åˆ›å»ºSitemapè§£æå™¨å®ä¾‹

    Args:
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        SitemapParser: è§£æå™¨å®ä¾‹
    """
    return SitemapParser(timeout=timeout, max_retries=max_retries, cache_ttl=cache_ttl)

if __name__ == '__main__':
    import sys
    import json
    from datetime import datetime

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(funcName)s:%(lineno)d] - %(message)s',
        level=logging.INFO
    )

    def datetime_handler(x):
        if isinstance(x, datetime):
            return x.isoformat()
        raise TypeError(f"Object of type {type(x)} is not JSON serializable")

    if len(sys.argv) != 2:
        logger.error("Usage: python sitemap_parser.py <sitemap_url>")
        logger.error("Example: python sitemap_parser.py https://example.com/sitemap.xml")
        sys.exit(1)

    url = sys.argv[1]
    logger.info(f"æ­£åœ¨è§£æSitemap: {url}")

    parser = create_sitemap_parser()
    entries = parser.parse(url)

    logger.info(f"\nè¿”å› {len(entries)} ä¸ªæ¡ç›® (å·²é™åˆ¶ä¸ºæœ€è¿‘çš„10ä¸ª):")
    for i, entry in enumerate(entries, 1):
        logger.info(f"\næ¡ç›® {i}:")
        logger.info(f"URL: {entry.url}")
        if entry.last_modified:
            logger.info(f"æœ€åä¿®æ”¹æ—¶é—´: {entry.last_modified.isoformat()}")

    # è¾“å‡ºJSONæ ¼å¼çš„å®Œæ•´ç»“æœ
    logger.info("\nå®Œæ•´JSONç»“æœ:")
    logger.info(json.dumps([{
        'url': entry.url,
        'last_modified': entry.last_modified.isoformat() if entry.last_modified else None
    } for entry in entries], indent=2, ensure_ascii=False, default=datetime_handler))