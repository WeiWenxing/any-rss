"""
RSSè§£æå™¨æ¨¡å—

è¯¥æ¨¡å—è´Ÿè´£ä»RSS/Atom XMLè§£æå‡ºæ ‡å‡†åŒ–çš„RSSEntryå¯¹è±¡ã€‚
æ”¯æŒRSS 2.0å’ŒAtom 1.0æ ¼å¼ï¼Œæä¾›ç»Ÿä¸€çš„è§£ææ¥å£ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. RSS/Atom XMLçš„è§£æå’Œå¤„ç†
2. åª’ä½“é™„ä»¶çš„æå–å’Œå¤„ç†
3. å†…å®¹æ¸…ç†å’Œæ ¼å¼åŒ–
4. é”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶
5. ç¼–ç æ£€æµ‹å’Œå¤„ç†

ä½œè€…: Assistant
åˆ›å»ºæ—¶é—´: 2024å¹´
"""

import logging
import re
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
from urllib.parse import urljoin, urlparse
import feedparser
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import hashlib

from .rss_entry import RSSEntry, RSSEnclosure, create_rss_entry
from services.common.cache import get_cache


class RSSParser:
    """
    RSSè§£æå™¨

    è´Ÿè´£ä»RSS/Atom XMLè§£æå‡ºæ ‡å‡†åŒ–çš„RSSEntryå¯¹è±¡åˆ—è¡¨
    """

    def __init__(self, timeout: int = 30, max_retries: int = 3, cache_ttl: int = 21600):
        """
        åˆå§‹åŒ–RSSè§£æå™¨

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤6å°æ—¶
        """
        self.logger = logging.getLogger(__name__)
        self.timeout = timeout
        self.max_retries = max_retries

        # åˆå§‹åŒ–ç¼“å­˜
        self.cache = get_cache("rsshub_parser", ttl=cache_ttl)

        # é…ç½®HTTPä¼šè¯
        self.session = requests.Session()

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # è®¾ç½®å®Œå–„çš„è¯·æ±‚å¤´ï¼ˆä¸æ™®é€šRSSæ¨¡å—ä¿æŒä¸€è‡´ï¼‰
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml, application/atom+xml, application/xml, text/xml, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache'
        })

        self.logger.info(f"RSSè§£æå™¨åˆå§‹åŒ–å®Œæˆï¼Œè¶…æ—¶: {timeout}s, é‡è¯•: {max_retries}æ¬¡, ç¼“å­˜TTL: {cache_ttl}s")

    def _generate_cache_key(self, rss_url: str) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            rss_url: RSSæºURL

        Returns:
            str: ç¼“å­˜é”®
        """
        # ä½¿ç”¨URLç”Ÿæˆå”¯ä¸€çš„ç¼“å­˜é”®
        cache_key = hashlib.md5(rss_url.encode('utf-8')).hexdigest()
        return f"rss_feed:{cache_key}"

    def parse_feed(self, rss_url: str) -> List[RSSEntry]:
        """
        è§£æRSSæºï¼Œè¿”å›RSSæ¡ç›®åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            rss_url: RSSæºURL

        Returns:
            List[RSSEntry]: RSSæ¡ç›®åˆ—è¡¨

        Raises:
            Exception: è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(rss_url)

            # å°è¯•ä»ç¼“å­˜è·å–æ•°æ®
            cached_data = self.cache.get(cache_key)
            if cached_data is not None:
                self.logger.info(f"ğŸ“¦ ä»ç¼“å­˜è·å–RSSå†…å®¹: {rss_url}, æ¡ç›®æ•°: {len(cached_data)}")
                # å°†ç¼“å­˜çš„å­—å…¸æ•°æ®è½¬æ¢å›RSSEntryå¯¹è±¡
                entries = []
                for entry_dict in cached_data:
                    try:
                        entry = self._dict_to_rss_entry(entry_dict)
                        if entry:
                            entries.append(entry)
                    except Exception as e:
                        self.logger.warning(f"ç¼“å­˜æ¡ç›®è½¬æ¢å¤±è´¥: {str(e)}")
                        continue
                return entries

            self.logger.info(f"ğŸŒ å¼€å§‹è§£æRSSæº: {rss_url}")

            # è·å–RSSå†…å®¹
            rss_content = self._fetch_rss_content(rss_url)

            # è§£æRSSå†…å®¹
            entries = self._parse_rss_content(rss_content, rss_url)

            # ç¼“å­˜è§£æç»“æœï¼ˆè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼‰
            if entries:
                cache_data = []
                for entry in entries:
                    try:
                        entry_dict = self._rss_entry_to_dict(entry)
                        cache_data.append(entry_dict)
                    except Exception as e:
                        self.logger.warning(f"æ¡ç›®åºåˆ—åŒ–å¤±è´¥: {str(e)}")
                        continue

                self.cache.set(cache_key, cache_data)
                self.logger.info(f"ğŸ’¾ RSSå†…å®¹å·²ç¼“å­˜: {rss_url}, æ¡ç›®æ•°: {len(cache_data)}")

            self.logger.info(f"RSSè§£æå®Œæˆ: {rss_url}, è·å–åˆ° {len(entries)} ä¸ªæ¡ç›®")
            return entries

        except Exception as e:
            error_msg = f"RSSè§£æå¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            raise Exception(error_msg)

    def _fetch_rss_content(self, rss_url: str) -> str:
        """
        è·å–RSSå†…å®¹

        Args:
            rss_url: RSSæºURL

        Returns:
            str: RSS XMLå†…å®¹
        """
        try:
            self.logger.debug(f"è·å–RSSå†…å®¹: {rss_url}")

            response = self.session.get(rss_url, timeout=self.timeout)
            response.raise_for_status()

            # æ£€æµ‹ç¼–ç 
            content = response.content
            encoding = response.encoding or 'utf-8'

            # å°è¯•è§£ç 
            try:
                rss_content = content.decode(encoding)
            except UnicodeDecodeError:
                # å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                for fallback_encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                    try:
                        rss_content = content.decode(fallback_encoding)
                        self.logger.debug(f"ä½¿ç”¨å¤‡ç”¨ç¼–ç  {fallback_encoding} è§£ç æˆåŠŸ")
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†
                    rss_content = content.decode('utf-8', errors='ignore')
                    self.logger.warning(f"ç¼–ç æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¿½ç•¥é”™è¯¯æ¨¡å¼")

            self.logger.debug(f"RSSå†…å®¹è·å–æˆåŠŸ: {len(rss_content)} å­—ç¬¦")
            return rss_content

        except Exception as e:
            self.logger.error(f"è·å–RSSå†…å®¹å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
            raise

    def _parse_rss_content(self, rss_content: str, rss_url: str) -> List[RSSEntry]:
        """
        è§£æRSSå†…å®¹

        Args:
            rss_content: RSS XMLå†…å®¹
            rss_url: RSSæºURL

        Returns:
            List[RSSEntry]: RSSæ¡ç›®åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨feedparserè§£æRSS
            feed = feedparser.parse(rss_content)

            if feed.bozo and feed.bozo_exception:
                self.logger.warning(f"RSSæ ¼å¼è­¦å‘Š: {feed.bozo_exception}")

            # è·å–æºä¿¡æ¯
            source_title = getattr(feed.feed, 'title', None)

            entries = []
            for entry_data in feed.entries:
                try:
                    entry = self._parse_single_entry(entry_data, rss_url, source_title)
                    if entry:
                        entries.append(entry)
                except Exception as e:
                    self.logger.warning(f"è§£æå•ä¸ªæ¡ç›®å¤±è´¥: {str(e)}")
                    continue

            self.logger.debug(f"æˆåŠŸè§£æ {len(entries)} ä¸ªRSSæ¡ç›®")
            return entries

        except Exception as e:
            self.logger.error(f"è§£æRSSå†…å®¹å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _parse_single_entry(self, entry_data: Any, rss_url: str, source_title: Optional[str]) -> Optional[RSSEntry]:
        """
        è§£æå•ä¸ªRSSæ¡ç›®

        Args:
            entry_data: feedparserè§£æçš„æ¡ç›®æ•°æ®
            rss_url: RSSæºURL
            source_title: RSSæºæ ‡é¢˜

        Returns:
            Optional[RSSEntry]: RSSæ¡ç›®å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å›None
        """
        try:
            # æå–åŸºç¡€ä¿¡æ¯
            title = getattr(entry_data, 'title', '').strip()
            link = getattr(entry_data, 'link', '').strip()

            if not title or not link:
                self.logger.warning(f"æ¡ç›®ç¼ºå°‘å¿…è¦ä¿¡æ¯: title='{title}', link='{link}'")
                return None

            # æå–æè¿°
            description = self._extract_description(entry_data)

            # æå–GUID
            guid = getattr(entry_data, 'id', None) or getattr(entry_data, 'guid', None)

            # æå–æ—¶é—´
            published = self._parse_datetime(getattr(entry_data, 'published_parsed', None))
            updated = self._parse_datetime(getattr(entry_data, 'updated_parsed', None))

            # æå–ä½œè€…
            author = self._extract_author(entry_data)

            # æå–åˆ†ç±»
            category = self._extract_category(entry_data)

            # æå–å†…å®¹
            content = self._extract_content(entry_data)
            summary = getattr(entry_data, 'summary', None)

            # åˆ›å»ºRSSæ¡ç›®
            entry = create_rss_entry(
                title=title,
                link=link,
                description=description,
                guid=guid,
                published=published,
                updated=updated,
                author=author,
                category=category,
                content=content,
                summary=summary,
                source_url=rss_url,
                source_title=source_title,
                raw_data=entry_data
            )

            # æå–åª’ä½“é™„ä»¶
            self._extract_enclosures(entry_data, entry)

            # ä»å†…å®¹ä¸­æå–é¢å¤–çš„åª’ä½“
            self._extract_media_from_content(entry)

            return entry

        except Exception as e:
            self.logger.error(f"è§£æå•ä¸ªæ¡ç›®å¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_description(self, entry_data: Any) -> str:
        """æå–æ¡ç›®æè¿°"""
        # å°è¯•å¤šä¸ªå­—æ®µ
        for field in ['description', 'summary', 'subtitle']:
            value = getattr(entry_data, field, None)
            if value:
                return self._clean_html(value).strip()
        return ""

    def _extract_author(self, entry_data: Any) -> Optional[str]:
        """æå–ä½œè€…ä¿¡æ¯"""
        # å°è¯•å¤šä¸ªå­—æ®µ
        author = getattr(entry_data, 'author', None)
        if author:
            return author.strip()

        # å°è¯•author_detail
        author_detail = getattr(entry_data, 'author_detail', None)
        if author_detail and hasattr(author_detail, 'name'):
            return author_detail.name.strip()

        return None

    def _extract_category(self, entry_data: Any) -> Optional[str]:
        """æå–åˆ†ç±»ä¿¡æ¯"""
        tags = getattr(entry_data, 'tags', [])
        if tags:
            # å–ç¬¬ä¸€ä¸ªæ ‡ç­¾ä½œä¸ºåˆ†ç±»
            first_tag = tags[0]
            if hasattr(first_tag, 'term'):
                return first_tag.term.strip()
            elif isinstance(first_tag, str):
                return first_tag.strip()

        return None

    def _extract_content(self, entry_data: Any) -> Optional[str]:
        """æå–å®Œæ•´å†…å®¹"""
        # å°è¯•contentå­—æ®µ
        content_list = getattr(entry_data, 'content', [])
        if content_list:
            # å–ç¬¬ä¸€ä¸ªcontent
            content_item = content_list[0]
            if hasattr(content_item, 'value'):
                return self._clean_html(content_item.value).strip()

        # å°è¯•content_encodedå­—æ®µï¼ˆRSSæ‰©å±•ï¼‰
        content_encoded = getattr(entry_data, 'content_encoded', None)
        if content_encoded:
            return self._clean_html(content_encoded).strip()

        return None

    def _extract_enclosures(self, entry_data: Any, entry: RSSEntry) -> None:
        """æå–åª’ä½“é™„ä»¶"""
        # å¤„ç†enclosureså­—æ®µ
        enclosures = getattr(entry_data, 'enclosures', [])
        for enclosure in enclosures:
            try:
                url = getattr(enclosure, 'href', '') or getattr(enclosure, 'url', '')
                mime_type = getattr(enclosure, 'type', '')
                length = getattr(enclosure, 'length', None)

                if url and mime_type:
                    # è½¬æ¢é•¿åº¦ä¸ºæ•´æ•°
                    if length:
                        try:
                            length = int(length)
                        except (ValueError, TypeError):
                            length = None

                    entry.add_enclosure(url, mime_type, length)

            except Exception as e:
                self.logger.warning(f"å¤„ç†enclosureå¤±è´¥: {str(e)}")
                continue

    def _extract_media_from_content(self, entry: RSSEntry) -> None:
        """ä»å†…å®¹ä¸­æå–åª’ä½“é“¾æ¥ï¼ˆå‚è€ƒæ™®é€šRSSæ¨¡å—çš„ç­–ç•¥ï¼‰"""
        # è·å–åŸå§‹HTMLå†…å®¹ï¼ˆæœªæ¸…ç†çš„ï¼‰
        raw_content = None

        # å°è¯•ä»åŸå§‹æ•°æ®ä¸­è·å–HTMLå†…å®¹
        if hasattr(entry, 'raw_data') and entry.raw_data:
            # å°è¯•contentå­—æ®µ
            content_list = getattr(entry.raw_data, 'content', [])
            if content_list:
                content_item = content_list[0]
                if hasattr(content_item, 'value'):
                    raw_content = content_item.value

            # å¦‚æœæ²¡æœ‰contentï¼Œå°è¯•content_encoded
            if not raw_content:
                raw_content = getattr(entry.raw_data, 'content_encoded', None)

            # å¦‚æœè¿˜æ²¡æœ‰ï¼Œå°è¯•description/summary
            if not raw_content:
                raw_content = getattr(entry.raw_data, 'description', None) or getattr(entry.raw_data, 'summary', None)

        # å¦‚æœæ²¡æœ‰åŸå§‹æ•°æ®ï¼Œä½¿ç”¨å·²æœ‰çš„æœ‰æ•ˆå†…å®¹ï¼ˆè™½ç„¶å¯èƒ½å·²è¢«æ¸…ç†ï¼‰
        if not raw_content:
            raw_content = entry.effective_content

        if not raw_content:
            return

        self.logger.debug(f"ä»å†…å®¹ä¸­æå–åª’ä½“ï¼ŒåŸå§‹å†…å®¹é•¿åº¦: {len(raw_content)} å­—ç¬¦")

        # ä½¿ç”¨BeautifulSoupè§£æHTMLï¼ˆå‚è€ƒæ™®é€šRSSæ¨¡å—çš„ç­–ç•¥ï¼‰
        try:
            from bs4 import BeautifulSoup

            # è§£æHTMLå†…å®¹
            soup = BeautifulSoup(raw_content, 'html.parser')

            # æå–å›¾ç‰‡
            img_tags = soup.find_all('img', src=True)
            self.logger.debug(f"ä½¿ç”¨BeautifulSoupæ‰¾åˆ° {len(img_tags)} ä¸ªimgæ ‡ç­¾")

            for img_tag in img_tags:
                try:
                    img_url = img_tag.get('src', '').strip()
                    if not img_url or not img_url.startswith(('http://', 'https://')):
                        continue

                    # è¿‡æ»¤è£…é¥°å›¾ç‰‡ï¼ˆå‚è€ƒæ™®é€šRSSæ¨¡å—çš„ç­–ç•¥ï¼‰
                    if any(keyword in img_url.lower() for keyword in ['icon', 'logo', 'avatar', 'emoji', 'button']):
                        self.logger.debug(f"è¿‡æ»¤è£…é¥°å›¾ç‰‡: {img_url}")
                        continue

                    # è½¬æ¢ä¸ºç»å¯¹URL
                    absolute_url = entry.get_absolute_url(img_url)

                    # æ·»åŠ ä¸ºå›¾ç‰‡é™„ä»¶
                    entry.add_enclosure(absolute_url, 'image/jpeg')
                    self.logger.debug(f"ä»å†…å®¹ä¸­æ·»åŠ å›¾ç‰‡é™„ä»¶: {absolute_url}")

                except Exception as e:
                    self.logger.debug(f"å¤„ç†å†…å®¹å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {str(e)}")
                    continue

            # æå–è§†é¢‘ï¼ˆå‚è€ƒæ™®é€šRSSæ¨¡å—çš„ç­–ç•¥ï¼‰
            video_tags = soup.find_all('video', src=True)
            self.logger.debug(f"ä½¿ç”¨BeautifulSoupæ‰¾åˆ° {len(video_tags)} ä¸ªvideoæ ‡ç­¾")

            for video_tag in video_tags:
                try:
                    video_url = video_tag.get('src', '').strip()
                    if not video_url or not video_url.startswith(('http://', 'https://')):
                        continue

                    # è½¬æ¢ä¸ºç»å¯¹URL
                    absolute_url = entry.get_absolute_url(video_url)

                    # æ·»åŠ ä¸ºè§†é¢‘é™„ä»¶
                    entry.add_enclosure(absolute_url, 'video/mp4')
                    self.logger.debug(f"ä»å†…å®¹ä¸­æ·»åŠ è§†é¢‘é™„ä»¶: {absolute_url}")

                except Exception as e:
                    self.logger.debug(f"å¤„ç†å†…å®¹è§†é¢‘å¤±è´¥: {video_url}, é”™è¯¯: {str(e)}")
                    continue

        except ImportError:
            self.logger.warning("BeautifulSoupä¸å¯ç”¨ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼è§£æ")
            # å›é€€åˆ°åŸæ¥çš„æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•
            img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
            img_matches = re.findall(img_pattern, raw_content, re.IGNORECASE)

            self.logger.debug(f"æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ° {len(img_matches)} ä¸ªimgæ ‡ç­¾")

            for img_url in img_matches:
                try:
                    # è¿‡æ»¤è£…é¥°å›¾ç‰‡
                    if any(keyword in img_url.lower() for keyword in ['icon', 'logo', 'avatar', 'emoji', 'button']):
                        self.logger.debug(f"è¿‡æ»¤è£…é¥°å›¾ç‰‡: {img_url}")
                        continue

                    # è½¬æ¢ä¸ºç»å¯¹URL
                    absolute_url = entry.get_absolute_url(img_url)

                    # æ·»åŠ ä¸ºå›¾ç‰‡é™„ä»¶
                    entry.add_enclosure(absolute_url, 'image/jpeg')
                    self.logger.debug(f"ä»å†…å®¹ä¸­æ·»åŠ å›¾ç‰‡é™„ä»¶: {absolute_url}")

                except Exception as e:
                    self.logger.debug(f"å¤„ç†å†…å®¹å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {str(e)}")
                    continue

        except Exception as e:
            self.logger.warning(f"åª’ä½“æå–å¤±è´¥: {str(e)}")
            return

    def _parse_datetime(self, time_struct) -> Optional[datetime]:
        """è§£ææ—¶é—´ç»“æ„"""
        if not time_struct:
            return None

        try:
            return datetime(*time_struct[:6])
        except (TypeError, ValueError):
            return None

    def _clean_html(self, html_content: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾"""
        if not html_content:
            return ""

        # ç®€å•çš„HTMLæ ‡ç­¾æ¸…ç†
        clean_text = re.sub(r'<[^>]+>', '', html_content)

        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text)

        return clean_text.strip()

    def validate_rss_url(self, rss_url: str) -> bool:
        """
        éªŒè¯RSS URLæ˜¯å¦æœ‰æ•ˆï¼ˆå®½æ¾éªŒè¯ï¼Œä¸æ™®é€šRSSæ¨¡å—ä¿æŒä¸€è‡´ï¼‰

        Args:
            rss_url: RSSæºURL

        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # åŸºç¡€URLæ ¼å¼éªŒè¯
            parsed = urlparse(rss_url)
            if not parsed.scheme or not parsed.netloc:
                self.logger.debug(f"RSS URLæ ¼å¼éªŒè¯å¤±è´¥: ç¼ºå°‘åè®®æˆ–åŸŸå - {rss_url}")
                return False

            # æ£€æŸ¥åè®®
            if parsed.scheme not in ['http', 'https']:
                self.logger.debug(f"RSS URLåè®®éªŒè¯å¤±è´¥: ä¸æ”¯æŒçš„åè®® {parsed.scheme} - {rss_url}")
                return False

            # å®½æ¾éªŒè¯ï¼šå°è¯•ç›´æ¥è§£æRSSå†…å®¹ï¼ˆä¸ä¾èµ–Content-Typeï¼‰
            try:
                self.logger.debug(f"å¼€å§‹å®½æ¾éªŒè¯RSSæº: {rss_url}")
                entries = self.parse_feed(rss_url)
                self.logger.debug(f"RSSæºéªŒè¯æˆåŠŸ: è§£æåˆ° {len(entries)} ä¸ªæ¡ç›® - {rss_url}")
                return True  # èƒ½è§£æå°±è®¤ä¸ºæœ‰æ•ˆ
            except Exception as parse_error:
                self.logger.debug(f"RSSå†…å®¹è§£æå¤±è´¥: {rss_url}, é”™è¯¯: {str(parse_error)}")

                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„è¿é€šæ€§æ£€æŸ¥
                try:
                    self.logger.debug(f"å°è¯•è¿é€šæ€§æ£€æŸ¥: {rss_url}")
                    response = self.session.head(rss_url, timeout=10)
                    response.raise_for_status()
                    self.logger.debug(f"è¿é€šæ€§æ£€æŸ¥é€šè¿‡ï¼Œå‡è®¾RSSæºæœ‰æ•ˆ: {rss_url}")
                    return True  # è¿é€šæ€§æ­£å¸¸ï¼Œå‡è®¾RSSæºæœ‰æ•ˆ
                except Exception as conn_error:
                    self.logger.debug(f"è¿é€šæ€§æ£€æŸ¥ä¹Ÿå¤±è´¥: {rss_url}, é”™è¯¯: {str(conn_error)}")
                    return False

        except Exception as e:
            self.logger.debug(f"RSS URLéªŒè¯å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}")
            return False

    def get_feed_info(self, rss_url: str) -> Dict[str, Any]:
        """
        è·å–RSSæºä¿¡æ¯

        Args:
            rss_url: RSSæºURL

        Returns:
            Dict[str, Any]: RSSæºä¿¡æ¯
        """
        try:
            rss_content = self._fetch_rss_content(rss_url)
            feed = feedparser.parse(rss_content)

            return {
                'title': getattr(feed.feed, 'title', ''),
                'description': getattr(feed.feed, 'description', ''),
                'link': getattr(feed.feed, 'link', ''),
                'language': getattr(feed.feed, 'language', ''),
                'updated': self._parse_datetime(getattr(feed.feed, 'updated_parsed', None)),
                'entry_count': len(feed.entries),
                'version': feed.version
            }

        except Exception as e:
            self.logger.error(f"è·å–RSSæºä¿¡æ¯å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
            return {}

    def _rss_entry_to_dict(self, entry: RSSEntry) -> Dict[str, Any]:
        """
        å°†RSSEntryå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆç”¨äºç¼“å­˜ï¼‰

        Args:
            entry: RSSEntryå¯¹è±¡

        Returns:
            Dict[str, Any]: å­—å…¸æ ¼å¼çš„æ¡ç›®æ•°æ®
        """
        try:
            entry_dict = {
                'title': entry.title,
                'link': entry.link,
                'description': entry.description,
                'guid': entry.guid,
                'published': entry.published.isoformat() if entry.published else None,
                'updated': entry.updated.isoformat() if entry.updated else None,
                'author': entry.author,
                'category': entry.category,
                'content': entry.content,
                'summary': entry.summary,
                'source_url': entry.source_url,
                'source_title': entry.source_title,
                'enclosures': []
            }

            # åºåˆ—åŒ–é™„ä»¶
            for enclosure in entry.enclosures:
                enclosure_dict = {
                    'url': enclosure.url,
                    'mime_type': enclosure.type,
                    'length': enclosure.length
                }
                entry_dict['enclosures'].append(enclosure_dict)

            return entry_dict

        except Exception as e:
            self.logger.error(f"RSSEntryåºåˆ—åŒ–å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _dict_to_rss_entry(self, entry_dict: Dict[str, Any]) -> Optional[RSSEntry]:
        """
        å°†å­—å…¸æ ¼å¼è½¬æ¢ä¸ºRSSEntryå¯¹è±¡ï¼ˆä»ç¼“å­˜æ¢å¤ï¼‰

        Args:
            entry_dict: å­—å…¸æ ¼å¼çš„æ¡ç›®æ•°æ®

        Returns:
            Optional[RSSEntry]: RSSEntryå¯¹è±¡
        """
        try:
            # è§£ææ—¶é—´
            published = None
            if entry_dict.get('published'):
                try:
                    published = datetime.fromisoformat(entry_dict['published'])
                except ValueError:
                    pass

            updated = None
            if entry_dict.get('updated'):
                try:
                    updated = datetime.fromisoformat(entry_dict['updated'])
                except ValueError:
                    pass

            # åˆ›å»ºRSSEntryå¯¹è±¡
            entry = create_rss_entry(
                title=entry_dict.get('title', ''),
                link=entry_dict.get('link', ''),
                description=entry_dict.get('description', ''),
                guid=entry_dict.get('guid'),
                published=published,
                updated=updated,
                author=entry_dict.get('author'),
                category=entry_dict.get('category'),
                content=entry_dict.get('content'),
                summary=entry_dict.get('summary'),
                source_url=entry_dict.get('source_url'),
                source_title=entry_dict.get('source_title')
            )

            # æ¢å¤é™„ä»¶
            for enclosure_dict in entry_dict.get('enclosures', []):
                try:
                    entry.add_enclosure(
                        url=enclosure_dict.get('url', ''),
                        mime_type=enclosure_dict.get('mime_type', ''),
                        length=enclosure_dict.get('length')
                    )
                except Exception as e:
                    self.logger.warning(f"æ¢å¤é™„ä»¶å¤±è´¥: {str(e)}")
                    continue

            return entry

        except Exception as e:
            self.logger.error(f"å­—å…¸è½¬RSSEntryå¤±è´¥: {str(e)}", exc_info=True)
            return None

    def clear_cache(self, rss_url: str = None) -> bool:
        """
        æ¸…é™¤ç¼“å­˜

        Args:
            rss_url: æŒ‡å®šURLçš„ç¼“å­˜ï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            if rss_url:
                # æ¸…é™¤æŒ‡å®šURLçš„ç¼“å­˜
                cache_key = self._generate_cache_key(rss_url)
                success = self.cache.delete(cache_key)
                self.logger.info(f"æ¸…é™¤æŒ‡å®šURLç¼“å­˜: {rss_url}, æˆåŠŸ: {success}")
                return success
            else:
                # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
                success = self.cache.clear()
                self.logger.info(f"æ¸…é™¤æ‰€æœ‰RSSè§£æå™¨ç¼“å­˜, æˆåŠŸ: {success}")
                return success
        except Exception as e:
            self.logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}", exc_info=True)
            return False

    def get_cache_info(self) -> Dict:
        """
        è·å–ç¼“å­˜ä¿¡æ¯

        Returns:
            Dict: ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            return self.cache.get_info()
        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
            return {"error": str(e)}

    def is_cache_hit(self, rss_url: str) -> bool:
        """
        æ£€æŸ¥æŒ‡å®šURLæ˜¯å¦æœ‰ç¼“å­˜

        Args:
            rss_url: RSSæºURL

        Returns:
            bool: æ˜¯å¦æœ‰ç¼“å­˜
        """
        try:
            cache_key = self._generate_cache_key(rss_url)
            return self.cache.exists(cache_key)
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥ç¼“å­˜å¤±è´¥: {str(e)}", exc_info=True)
            return False


# ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºRSSè§£æå™¨å®ä¾‹
def create_rss_parser(timeout: int = 30, max_retries: int = 3, cache_ttl: int = 21600) -> RSSParser:
    """
    åˆ›å»ºRSSè§£æå™¨å®ä¾‹

    Args:
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        cache_ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤6å°æ—¶

    Returns:
        RSSParser: RSSè§£æå™¨å®ä¾‹
    """
    return RSSParser(timeout, max_retries, cache_ttl)


# ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿè§£æRSS
def parse_rss_feed(rss_url: str, timeout: int = 30) -> List[RSSEntry]:
    """
    å¿«é€Ÿè§£æRSSæºçš„ä¾¿æ·å‡½æ•°

    Args:
        rss_url: RSSæºURL
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´

    Returns:
        List[RSSEntry]: RSSæ¡ç›®åˆ—è¡¨
    """
    parser = create_rss_parser(timeout=timeout)
    return parser.parse_feed(rss_url)


if __name__ == "__main__":
    # æ¨¡å—æµ‹è¯•ä»£ç 
    def test_rss_parser():
        """æµ‹è¯•RSSè§£æå™¨åŠŸèƒ½"""
        print("ğŸ§ª RSSè§£æå™¨æ¨¡å—æµ‹è¯•")

        # åˆ›å»ºè§£æå™¨
        parser = create_rss_parser()
        print(f"âœ… åˆ›å»ºRSSè§£æå™¨: {type(parser).__name__}")

        # æµ‹è¯•URLéªŒè¯
        test_urls = [
            "https://feeds.bbci.co.uk/news/rss.xml",
            "https://invalid-url",
            "not-a-url"
        ]

        for url in test_urls:
            is_valid = parser.validate_rss_url(url)
            print(f"âœ… URLéªŒè¯ {url}: {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'}")

        print("ğŸ‰ RSSè§£æå™¨æ¨¡å—æµ‹è¯•å®Œæˆ")

    # è¿è¡Œæµ‹è¯•
    test_rss_parser()