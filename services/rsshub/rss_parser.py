"""
RSSè§£æå™¨æ¨¡å—

è¯¥æ¨¡å—è´Ÿè´£ä»RSS/Atom XMLè§£æå‡ºæ ‡å‡†åŒ–çš„RSSEntryå¯¹è±¡ã€‚
æ”¯æŒRSS 2.0å’ŒAtom 1.0æ ¼å¼ï¼Œæä¾›ç»Ÿä¸€çš„è§£ææ¥å£ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. RSS/Atom XMLçš„è§£æå’Œå¤„ç†
2. åª’ä½“é™„ä»¶çš„æå–å’Œå¤„ç†
3. å†…å®¹æ¸…ç†å’Œæ ¼å¼åŒ–
4. é”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶
5. ç¼–ç æ£€æµ‹å’Œå¤„ç†

ä½œè€…: Assistant
åˆ›å»ºæ—¶é—´: 2024å¹´
"""

import logging
import re
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Optional, Dict, Any, Union
from urllib.parse import urljoin, urlparse
import feedparser
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from .rss_entry import RSSEntry, RSSEnclosure, create_rss_entry


class RSSParser:
    """
    RSSè§£æå™¨

    è´Ÿè´£ä»RSS/Atom XMLè§£æå‡ºæ ‡å‡†åŒ–çš„RSSEntryå¯¹è±¡åˆ—è¡¨
    """

    def __init__(self, timeout: int = 30, max_retries: int = 3):
        """
        åˆå§‹åŒ–RSSè§£æå™¨

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.logger = logging.getLogger(__name__)
        self.timeout = timeout
        self.max_retries = max_retries

        # é…ç½®HTTPä¼šè¯
        self.session = requests.Session()

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # è®¾ç½®User-Agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })

        self.logger.info(f"RSSè§£æå™¨åˆå§‹åŒ–å®Œæˆï¼Œè¶…æ—¶: {timeout}s, é‡è¯•: {max_retries}æ¬¡")

    def parse_feed(self, rss_url: str) -> List[RSSEntry]:
        """
        è§£æRSSæºï¼Œè¿”å›RSSæ¡ç›®åˆ—è¡¨

        Args:
            rss_url: RSSæºURL

        Returns:
            List[RSSEntry]: RSSæ¡ç›®åˆ—è¡¨

        Raises:
            Exception: è§£æå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            self.logger.info(f"å¼€å§‹è§£æRSSæº: {rss_url}")

            # è·å–RSSå†…å®¹
            rss_content = self._fetch_rss_content(rss_url)

            # è§£æRSSå†…å®¹
            entries = self._parse_rss_content(rss_content, rss_url)

            self.logger.info(f"RSSè§£æå®Œæˆ: {rss_url}, è·å–åˆ° {len(entries)} ä¸ªæ¡ç›®")
            return entries

        except Exception as e:
            error_msg = f"RSSè§£æå¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            raise Exception(error_msg)

    def _fetch_rss_content(self, rss_url: str) -> str:
        """
        è·å–RSSå†…å®¹

        Args:
            rss_url: RSSæºURL

        Returns:
            str: RSS XMLå†…å®¹
        """
        try:
            self.logger.debug(f"è·å–RSSå†…å®¹: {rss_url}")

            response = self.session.get(rss_url, timeout=self.timeout)
            response.raise_for_status()

            # æ£€æµ‹ç¼–ç 
            content = response.content
            encoding = response.encoding or 'utf-8'

            # å°è¯•è§£ç 
            try:
                rss_content = content.decode(encoding)
            except UnicodeDecodeError:
                # å¦‚æœè§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                for fallback_encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                    try:
                        rss_content = content.decode(fallback_encoding)
                        self.logger.debug(f"ä½¿ç”¨å¤‡ç”¨ç¼–ç  {fallback_encoding} è§£ç æˆåŠŸ")
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†
                    rss_content = content.decode('utf-8', errors='ignore')
                    self.logger.warning(f"ç¼–ç æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¿½ç•¥é”™è¯¯æ¨¡å¼")

            self.logger.debug(f"RSSå†…å®¹è·å–æˆåŠŸ: {len(rss_content)} å­—ç¬¦")
            return rss_content

        except Exception as e:
            self.logger.error(f"è·å–RSSå†…å®¹å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
            raise

    def _parse_rss_content(self, rss_content: str, rss_url: str) -> List[RSSEntry]:
        """
        è§£æRSSå†…å®¹

        Args:
            rss_content: RSS XMLå†…å®¹
            rss_url: RSSæºURL

        Returns:
            List[RSSEntry]: RSSæ¡ç›®åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨feedparserè§£æRSS
            feed = feedparser.parse(rss_content)

            if feed.bozo and feed.bozo_exception:
                self.logger.warning(f"RSSæ ¼å¼è­¦å‘Š: {feed.bozo_exception}")

            # è·å–æºä¿¡æ¯
            source_title = getattr(feed.feed, 'title', None)

            entries = []
            for entry_data in feed.entries:
                try:
                    entry = self._parse_single_entry(entry_data, rss_url, source_title)
                    if entry:
                        entries.append(entry)
                except Exception as e:
                    self.logger.warning(f"è§£æå•ä¸ªæ¡ç›®å¤±è´¥: {str(e)}")
                    continue

            self.logger.debug(f"æˆåŠŸè§£æ {len(entries)} ä¸ªRSSæ¡ç›®")
            return entries

        except Exception as e:
            self.logger.error(f"è§£æRSSå†…å®¹å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _parse_single_entry(self, entry_data: Any, rss_url: str, source_title: Optional[str]) -> Optional[RSSEntry]:
        """
        è§£æå•ä¸ªRSSæ¡ç›®

        Args:
            entry_data: feedparserè§£æçš„æ¡ç›®æ•°æ®
            rss_url: RSSæºURL
            source_title: RSSæºæ ‡é¢˜

        Returns:
            Optional[RSSEntry]: RSSæ¡ç›®å¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å›None
        """
        try:
            # æå–åŸºç¡€ä¿¡æ¯
            title = getattr(entry_data, 'title', '').strip()
            link = getattr(entry_data, 'link', '').strip()

            if not title or not link:
                self.logger.warning(f"æ¡ç›®ç¼ºå°‘å¿…è¦ä¿¡æ¯: title='{title}', link='{link}'")
                return None

            # æå–æè¿°
            description = self._extract_description(entry_data)

            # æå–GUID
            guid = getattr(entry_data, 'id', None) or getattr(entry_data, 'guid', None)

            # æå–æ—¶é—´
            published = self._parse_datetime(getattr(entry_data, 'published_parsed', None))
            updated = self._parse_datetime(getattr(entry_data, 'updated_parsed', None))

            # æå–ä½œè€…
            author = self._extract_author(entry_data)

            # æå–åˆ†ç±»
            category = self._extract_category(entry_data)

            # æå–å†…å®¹
            content = self._extract_content(entry_data)
            summary = getattr(entry_data, 'summary', None)

            # åˆ›å»ºRSSæ¡ç›®
            entry = create_rss_entry(
                title=title,
                link=link,
                description=description,
                guid=guid,
                published=published,
                updated=updated,
                author=author,
                category=category,
                content=content,
                summary=summary,
                source_url=rss_url,
                source_title=source_title,
                raw_data=entry_data
            )

            # æå–åª’ä½“é™„ä»¶
            self._extract_enclosures(entry_data, entry)

            # ä»å†…å®¹ä¸­æå–é¢å¤–çš„åª’ä½“
            self._extract_media_from_content(entry)

            return entry

        except Exception as e:
            self.logger.error(f"è§£æå•ä¸ªæ¡ç›®å¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_description(self, entry_data: Any) -> str:
        """æå–æ¡ç›®æè¿°"""
        # å°è¯•å¤šä¸ªå­—æ®µ
        for field in ['description', 'summary', 'subtitle']:
            value = getattr(entry_data, field, None)
            if value:
                return self._clean_html(value).strip()
        return ""

    def _extract_author(self, entry_data: Any) -> Optional[str]:
        """æå–ä½œè€…ä¿¡æ¯"""
        # å°è¯•å¤šä¸ªå­—æ®µ
        author = getattr(entry_data, 'author', None)
        if author:
            return author.strip()

        # å°è¯•author_detail
        author_detail = getattr(entry_data, 'author_detail', None)
        if author_detail and hasattr(author_detail, 'name'):
            return author_detail.name.strip()

        return None

    def _extract_category(self, entry_data: Any) -> Optional[str]:
        """æå–åˆ†ç±»ä¿¡æ¯"""
        tags = getattr(entry_data, 'tags', [])
        if tags:
            # å–ç¬¬ä¸€ä¸ªæ ‡ç­¾ä½œä¸ºåˆ†ç±»
            first_tag = tags[0]
            if hasattr(first_tag, 'term'):
                return first_tag.term.strip()
            elif isinstance(first_tag, str):
                return first_tag.strip()

        return None

    def _extract_content(self, entry_data: Any) -> Optional[str]:
        """æå–å®Œæ•´å†…å®¹"""
        # å°è¯•contentå­—æ®µ
        content_list = getattr(entry_data, 'content', [])
        if content_list:
            # å–ç¬¬ä¸€ä¸ªcontent
            content_item = content_list[0]
            if hasattr(content_item, 'value'):
                return self._clean_html(content_item.value).strip()

        # å°è¯•content_encodedå­—æ®µï¼ˆRSSæ‰©å±•ï¼‰
        content_encoded = getattr(entry_data, 'content_encoded', None)
        if content_encoded:
            return self._clean_html(content_encoded).strip()

        return None

    def _extract_enclosures(self, entry_data: Any, entry: RSSEntry) -> None:
        """æå–åª’ä½“é™„ä»¶"""
        # å¤„ç†enclosureså­—æ®µ
        enclosures = getattr(entry_data, 'enclosures', [])
        for enclosure in enclosures:
            try:
                url = getattr(enclosure, 'href', '') or getattr(enclosure, 'url', '')
                mime_type = getattr(enclosure, 'type', '')
                length = getattr(enclosure, 'length', None)

                if url and mime_type:
                    # è½¬æ¢é•¿åº¦ä¸ºæ•´æ•°
                    if length:
                        try:
                            length = int(length)
                        except (ValueError, TypeError):
                            length = None

                    entry.add_enclosure(url, mime_type, length)

            except Exception as e:
                self.logger.warning(f"å¤„ç†enclosureå¤±è´¥: {str(e)}")
                continue

    def _extract_media_from_content(self, entry: RSSEntry) -> None:
        """ä»å†…å®¹ä¸­æå–åª’ä½“é“¾æ¥"""
        content = entry.effective_content
        if not content:
            return

        # æå–å›¾ç‰‡é“¾æ¥
        img_pattern = r'<img[^>]+src=["\']([^"\']+)["\'][^>]*>'
        img_matches = re.findall(img_pattern, content, re.IGNORECASE)

        for img_url in img_matches:
            try:
                # è½¬æ¢ä¸ºç»å¯¹URL
                absolute_url = entry.get_absolute_url(img_url)

                # æ·»åŠ ä¸ºå›¾ç‰‡é™„ä»¶
                entry.add_enclosure(absolute_url, 'image/jpeg')

            except Exception as e:
                self.logger.debug(f"å¤„ç†å†…å®¹å›¾ç‰‡å¤±è´¥: {img_url}, é”™è¯¯: {str(e)}")
                continue

    def _parse_datetime(self, time_struct) -> Optional[datetime]:
        """è§£ææ—¶é—´ç»“æ„"""
        if not time_struct:
            return None

        try:
            return datetime(*time_struct[:6])
        except (TypeError, ValueError):
            return None

    def _clean_html(self, html_content: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾"""
        if not html_content:
            return ""

        # ç®€å•çš„HTMLæ ‡ç­¾æ¸…ç†
        clean_text = re.sub(r'<[^>]+>', '', html_content)

        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        clean_text = re.sub(r'\s+', ' ', clean_text)

        return clean_text.strip()

    def validate_rss_url(self, rss_url: str) -> bool:
        """
        éªŒè¯RSS URLæ˜¯å¦æœ‰æ•ˆ

        Args:
            rss_url: RSSæºURL

        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            # åŸºç¡€URLæ ¼å¼éªŒè¯
            parsed = urlparse(rss_url)
            if not parsed.scheme or not parsed.netloc:
                return False

            # å°è¯•è·å–RSSå†…å®¹
            response = self.session.head(rss_url, timeout=10)
            response.raise_for_status()

            # æ£€æŸ¥Content-Type
            content_type = response.headers.get('content-type', '').lower()
            valid_types = ['application/rss+xml', 'application/atom+xml', 'text/xml', 'application/xml']

            if any(valid_type in content_type for valid_type in valid_types):
                return True

            # å¦‚æœContent-Typeä¸æ˜ç¡®ï¼Œå°è¯•è§£æå†…å®¹
            try:
                entries = self.parse_feed(rss_url)
                return len(entries) >= 0  # èƒ½è§£æå°±è®¤ä¸ºæœ‰æ•ˆ
            except:
                return False

        except Exception as e:
            self.logger.debug(f"RSS URLéªŒè¯å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}")
            return False

    def get_feed_info(self, rss_url: str) -> Dict[str, Any]:
        """
        è·å–RSSæºä¿¡æ¯

        Args:
            rss_url: RSSæºURL

        Returns:
            Dict[str, Any]: RSSæºä¿¡æ¯
        """
        try:
            rss_content = self._fetch_rss_content(rss_url)
            feed = feedparser.parse(rss_content)

            return {
                'title': getattr(feed.feed, 'title', ''),
                'description': getattr(feed.feed, 'description', ''),
                'link': getattr(feed.feed, 'link', ''),
                'language': getattr(feed.feed, 'language', ''),
                'updated': self._parse_datetime(getattr(feed.feed, 'updated_parsed', None)),
                'entry_count': len(feed.entries),
                'version': feed.version
            }

        except Exception as e:
            self.logger.error(f"è·å–RSSæºä¿¡æ¯å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
            return {}


# ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºRSSè§£æå™¨å®ä¾‹
def create_rss_parser(timeout: int = 30, max_retries: int = 3) -> RSSParser:
    """
    åˆ›å»ºRSSè§£æå™¨å®ä¾‹

    Args:
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        RSSParser: RSSè§£æå™¨å®ä¾‹
    """
    return RSSParser(timeout, max_retries)


# ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿè§£æRSS
def parse_rss_feed(rss_url: str, timeout: int = 30) -> List[RSSEntry]:
    """
    å¿«é€Ÿè§£æRSSæºçš„ä¾¿æ·å‡½æ•°

    Args:
        rss_url: RSSæºURL
        timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´

    Returns:
        List[RSSEntry]: RSSæ¡ç›®åˆ—è¡¨
    """
    parser = create_rss_parser(timeout=timeout)
    return parser.parse_feed(rss_url)


if __name__ == "__main__":
    # æ¨¡å—æµ‹è¯•ä»£ç 
    def test_rss_parser():
        """æµ‹è¯•RSSè§£æå™¨åŠŸèƒ½"""
        print("ğŸ§ª RSSè§£æå™¨æ¨¡å—æµ‹è¯•")

        # åˆ›å»ºè§£æå™¨
        parser = create_rss_parser()
        print(f"âœ… åˆ›å»ºRSSè§£æå™¨: {type(parser).__name__}")

        # æµ‹è¯•URLéªŒè¯
        test_urls = [
            "https://feeds.bbci.co.uk/news/rss.xml",
            "https://invalid-url",
            "not-a-url"
        ]

        for url in test_urls:
            is_valid = parser.validate_rss_url(url)
            print(f"âœ… URLéªŒè¯ {url}: {'æœ‰æ•ˆ' if is_valid else 'æ— æ•ˆ'}")

        print("ğŸ‰ RSSè§£æå™¨æ¨¡å—æµ‹è¯•å®Œæˆ")

    # è¿è¡Œæµ‹è¯•
    test_rss_parser()