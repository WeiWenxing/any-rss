"""
RSSHubè°ƒåº¦å™¨æ¨¡å—

è¯¥æ¨¡å—è´Ÿè´£RSSHubçš„å®šæ—¶ä»»åŠ¡è°ƒåº¦ï¼Œå®Œå…¨å¤ç”¨douyinæ¨¡å—çš„è°ƒåº¦é€»è¾‘ã€‚
æ”¯æŒå®šæ—¶æ£€æŸ¥RSSæ›´æ–°ã€æ‰¹é‡å‘é€æ–°å†…å®¹ã€æ¸…ç†è¿‡æœŸæ•°æ®ç­‰åŠŸèƒ½ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. å®šæ—¶æ£€æŸ¥æ‰€æœ‰RSSæºçš„æ›´æ–°
2. æ‰¹é‡å‘é€æ–°RSSå†…å®¹åˆ°è®¢é˜…é¢‘é“
3. æ™ºèƒ½å»é‡å’Œå†…å®¹è¿‡æ»¤
4. å‘é€é—´éš”ç®¡ç†å’Œé”™è¯¯å¤„ç†
5. æ•°æ®æ¸…ç†å’Œç»´æŠ¤ä»»åŠ¡

ä½œè€…: Assistant
åˆ›å»ºæ—¶é—´: 2024å¹´
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
from telegram import Bot

from .manager import RSSHubManager, create_rsshub_manager
from .rss_parser import RSSParser, create_rss_parser
from .rss_converter import create_rss_converter
from .rss_entry import RSSEntry
from services.common.unified_sender import UnifiedTelegramSender
from services.common.unified_interval_manager import UnifiedIntervalManager


class RSSHubScheduler:
    """
    RSSHubå®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨

    å®Œå…¨å¤ç”¨douyinæ¨¡å—çš„è°ƒåº¦é€»è¾‘ï¼Œä¸ºRSSè®¢é˜…æä¾›å®šæ—¶æ›´æ–°å’Œå‘é€åŠŸèƒ½
    """

    def __init__(self, data_dir: str = "data/rsshub"):
        """
        åˆå§‹åŒ–RSSHubè°ƒåº¦å™¨

        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.rsshub_manager = create_rsshub_manager(data_dir)
        self.rss_parser = create_rss_parser()
        self.rss_converter = create_rss_converter()
        self.unified_sender = UnifiedTelegramSender()

        # è°ƒåº¦é…ç½®ï¼ˆå¤ç”¨douyinçš„é…ç½®é€»è¾‘ï¼‰
        self.check_interval = 300  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆä¸douyinä¿æŒä¸€è‡´ï¼‰
        self.max_concurrent_feeds = 5  # æœ€å¤§å¹¶å‘RSSæºæ•°é‡
        self.max_items_per_batch = 20  # æ¯æ‰¹æœ€å¤§æ¡ç›®æ•°é‡

        self.logger.info(f"RSSHubè°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ£€æŸ¥é—´éš”: {self.check_interval}ç§’")

    async def check_all_rss_updates(self, bot: Bot) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ‰€æœ‰RSSæºçš„æ›´æ–°ï¼ˆå®Œå…¨å¤ç”¨douyinçš„æ£€æŸ¥é€»è¾‘ï¼‰

        Args:
            bot: Telegram Botå®ä¾‹

        Returns:
            Dict[str, Any]: æ£€æŸ¥ç»“æœç»Ÿè®¡
        """
        try:
            self.logger.info("å¼€å§‹æ£€æŸ¥æ‰€æœ‰RSSæºæ›´æ–°")
            start_time = datetime.now()

            # è·å–æ‰€æœ‰RSSæº
            all_rss_urls = self.rsshub_manager.get_all_rss_urls()
            if not all_rss_urls:
                self.logger.info("æ²¡æœ‰RSSè®¢é˜…ï¼Œè·³è¿‡æ£€æŸ¥")
                return {"total_feeds": 0, "updated_feeds": 0, "new_items": 0, "sent_items": 0}

            self.logger.info(f"å¼€å§‹æ£€æŸ¥ {len(all_rss_urls)} ä¸ªRSSæº")

            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "total_feeds": len(all_rss_urls),
                "updated_feeds": 0,
                "new_items": 0,
                "sent_items": 0,
                "errors": 0
            }

            # åˆ†æ‰¹å¤„ç†RSSæºï¼ˆé¿å…å¹¶å‘è¿‡å¤šï¼‰
            for i in range(0, len(all_rss_urls), self.max_concurrent_feeds):
                batch_urls = all_rss_urls[i:i + self.max_concurrent_feeds]
                
                # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
                tasks = [
                    self._check_single_rss_update(bot, rss_url)
                    for rss_url in batch_urls
                ]

                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                # ç»Ÿè®¡æ‰¹æ¬¡ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        stats["errors"] += 1
                        self.logger.error(f"RSSæ£€æŸ¥ä»»åŠ¡å¼‚å¸¸: {str(result)}", exc_info=True)
                    elif isinstance(result, dict):
                        if result.get("has_updates"):
                            stats["updated_feeds"] += 1
                        stats["new_items"] += result.get("new_items", 0)
                        stats["sent_items"] += result.get("sent_items", 0)

            # è®°å½•æ£€æŸ¥å®Œæˆ
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            self.logger.info(
                f"RSSæ›´æ–°æ£€æŸ¥å®Œæˆ: "
                f"æ€»æºæ•°={stats['total_feeds']}, "
                f"æœ‰æ›´æ–°={stats['updated_feeds']}, "
                f"æ–°æ¡ç›®={stats['new_items']}, "
                f"å·²å‘é€={stats['sent_items']}, "
                f"é”™è¯¯={stats['errors']}, "
                f"è€—æ—¶={duration:.2f}ç§’"
            )

            return stats

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥RSSæ›´æ–°å¤±è´¥: {str(e)}", exc_info=True)
            return {"total_feeds": 0, "updated_feeds": 0, "new_items": 0, "sent_items": 0, "errors": 1}

    async def _check_single_rss_update(self, bot: Bot, rss_url: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•ä¸ªRSSæºçš„æ›´æ–°

        Args:
            bot: Telegram Botå®ä¾‹
            rss_url: RSSæºURL

        Returns:
            Dict[str, Any]: æ£€æŸ¥ç»“æœ
        """
        try:
            self.logger.debug(f"æ£€æŸ¥RSSæºæ›´æ–°: {rss_url}")

            # è·å–è®¢é˜…é¢‘é“
            target_channels = self.rsshub_manager.get_subscription_channels(rss_url)
            if not target_channels:
                self.logger.warning(f"RSSæºæ— è®¢é˜…é¢‘é“: {rss_url}")
                return {"has_updates": False, "new_items": 0, "sent_items": 0}

            # è§£æRSSå†…å®¹
            try:
                entries = self.rss_parser.parse_feed(rss_url)
            except Exception as e:
                self.logger.error(f"è§£æRSSæºå¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
                return {"has_updates": False, "new_items": 0, "sent_items": 0}

            if not entries:
                self.logger.debug(f"RSSæºæ— å†…å®¹: {rss_url}")
                return {"has_updates": False, "new_items": 0, "sent_items": 0}

            # è¿‡æ»¤æ–°å†…å®¹ï¼ˆå¤ç”¨douyinçš„å»é‡é€»è¾‘ï¼‰
            new_entries = self._filter_new_entries(rss_url, entries)

            if not new_entries:
                self.logger.debug(f"RSSæºæ— æ–°å†…å®¹: {rss_url}")
                return {"has_updates": False, "new_items": 0, "sent_items": 0}

            self.logger.info(f"RSSæºå‘ç°æ–°å†…å®¹: {rss_url}, {len(new_entries)} ä¸ªæ–°æ¡ç›®")

            # æ‰¹é‡å‘é€æ–°å†…å®¹
            sent_count = await self._process_batch(bot, new_entries, rss_url, target_channels)

            return {
                "has_updates": True,
                "new_items": len(new_entries),
                "sent_items": sent_count
            }

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥å•ä¸ªRSSæºå¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
            return {"has_updates": False, "new_items": 0, "sent_items": 0}

    def _filter_new_entries(self, rss_url: str, entries: List[RSSEntry]) -> List[RSSEntry]:
        """
        è¿‡æ»¤æ–°RSSæ¡ç›®ï¼ˆå®Œå…¨å¤ç”¨douyinçš„å»é‡é€»è¾‘ï¼‰

        Args:
            rss_url: RSSæºURL
            entries: RSSæ¡ç›®åˆ—è¡¨

        Returns:
            List[RSSEntry]: æ–°æ¡ç›®åˆ—è¡¨
        """
        try:
            # è·å–å·²çŸ¥æ¡ç›®ID
            known_item_ids = set(self.rsshub_manager.get_known_item_ids(rss_url))

            # è¿‡æ»¤æ–°æ¡ç›®
            new_entries = []
            for entry in entries:
                if entry.item_id not in known_item_ids:
                    new_entries.append(entry)

            # é™åˆ¶æ‰¹æ¬¡å¤§å°ï¼ˆé¿å…ä¸€æ¬¡å‘é€è¿‡å¤šï¼‰
            if len(new_entries) > self.max_items_per_batch:
                self.logger.warning(f"RSSæºæ–°æ¡ç›®è¿‡å¤š: {rss_url}, {len(new_entries)} ä¸ªï¼Œé™åˆ¶ä¸º {self.max_items_per_batch} ä¸ª")
                # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„æ¡ç›®
                sorted_entries = sorted(
                    new_entries, 
                    key=lambda x: x.effective_published_time or datetime.min, 
                    reverse=True
                )
                new_entries = sorted_entries[:self.max_items_per_batch]

            self.logger.debug(f"RSSæºè¿‡æ»¤ç»“æœ: {rss_url}, {len(new_entries)} ä¸ªæ–°æ¡ç›®")
            return new_entries

        except Exception as e:
            self.logger.error(f"è¿‡æ»¤RSSæ–°æ¡ç›®å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
            return []

    async def _process_batch(self, bot: Bot, new_entries: List[RSSEntry], rss_url: str, target_channels: List[str]) -> int:
        """
        å¤„ç†æ‰¹é‡RSSå†…å®¹ï¼Œä½¿ç”¨ç»Ÿä¸€çš„å‘é€ç­–ç•¥

        Args:
            bot: Telegram Botå®ä¾‹
            new_entries: æ–°RSSæ¡ç›®åˆ—è¡¨
            rss_url: RSSæºURL
            target_channels: ç›®æ ‡é¢‘é“åˆ—è¡¨

        Returns:
            int: å‘é€æˆåŠŸçš„å†…å®¹æ•°é‡
        """
        try:
            # åˆå§‹åŒ–é—´éš”ç®¡ç†å™¨ï¼ˆæ‰¹é‡å‘é€åœºæ™¯ï¼‰
            interval_manager = UnifiedIntervalManager("rsshub_send")

            # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆä»æ—§åˆ°æ–°ï¼Œç¡®ä¿æ—¶é—´é¡ºåºï¼‰
            sorted_entries = sorted(
                new_entries, 
                key=lambda x: x.effective_published_time or datetime.min, 
                reverse=False
            )

            sent_count = 0

            for entry in sorted_entries:
                try:
                    # è½¬æ¢ä¸ºç»Ÿä¸€æ¶ˆæ¯æ ¼å¼
                    telegram_message = self.rss_converter.to_telegram_message(entry)

                    # å‘é€åˆ°æ‰€æœ‰è®¢é˜…é¢‘é“
                    entry_sent_count = 0
                    for chat_id in target_channels:
                        try:
                            # å‘é€æ¶ˆæ¯
                            message_ids = await self.unified_sender.send_message(bot, chat_id, telegram_message)

                            if message_ids:
                                # ä¿å­˜æ¶ˆæ¯æ˜ å°„
                                self.rsshub_manager.save_message_mapping(rss_url, entry.item_id, chat_id, message_ids)
                                entry_sent_count += 1
                                self.logger.debug(f"RSSå†…å®¹å‘é€æˆåŠŸ: {entry.item_id} -> {chat_id}")

                            # åº”ç”¨å‘é€é—´éš”
                            await interval_manager.apply_interval()

                        except Exception as e:
                            self.logger.error(f"å‘é€RSSå†…å®¹åˆ°é¢‘é“å¤±è´¥: {chat_id}, æ¡ç›®: {entry.item_id}, é”™è¯¯: {str(e)}", exc_info=True)
                            continue

                    # å¦‚æœè‡³å°‘å‘é€åˆ°ä¸€ä¸ªé¢‘é“ï¼Œæ ‡è®°ä¸ºå·²çŸ¥æ¡ç›®
                    if entry_sent_count > 0:
                        self.rsshub_manager.add_known_item_id(rss_url, entry.item_id)
                        sent_count += 1
                        self.logger.debug(f"RSSæ¡ç›®å¤„ç†å®Œæˆ: {entry.item_id}, å‘é€åˆ° {entry_sent_count} ä¸ªé¢‘é“")

                except Exception as e:
                    self.logger.error(f"å¤„ç†RSSæ¡ç›®å¤±è´¥: {entry.item_id}, é”™è¯¯: {str(e)}", exc_info=True)
                    continue

            self.logger.info(f"RSSæ‰¹é‡å¤„ç†å®Œæˆ: {rss_url}, {sent_count}/{len(new_entries)} ä¸ªæ¡ç›®å‘é€æˆåŠŸ")
            return sent_count

        except Exception as e:
            self.logger.error(f"RSSæ‰¹é‡å¤„ç†å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}", exc_info=True)
            return 0

    def cleanup_old_data(self) -> None:
        """
        æ¸…ç†è¿‡æœŸçš„æ•°æ®ï¼ˆå¤ç”¨douyinçš„æ¸…ç†é€»è¾‘ï¼‰
        """
        try:
            self.logger.info("å¼€å§‹RSSæ•°æ®æ¸…ç†ä»»åŠ¡")

            # æ¸…ç†å­¤ç«‹çš„æ•°æ®
            cleaned_count = self.rsshub_manager.cleanup_orphaned_data()

            # æ¸…ç†è¿‡æœŸçš„å·²çŸ¥æ¡ç›®ï¼ˆä¿ç•™æœ€è¿‘1000ä¸ªï¼‰
            self._cleanup_old_known_items()

            self.logger.info(f"RSSæ•°æ®æ¸…ç†å®Œæˆï¼Œæ¸…ç†äº† {cleaned_count} ä¸ªå­¤ç«‹æ•°æ®é¡¹")

        except Exception as e:
            self.logger.error(f"RSSæ•°æ®æ¸…ç†å¤±è´¥: {str(e)}", exc_info=True)

    def _cleanup_old_known_items(self) -> None:
        """
        æ¸…ç†è¿‡æœŸçš„å·²çŸ¥æ¡ç›®IDï¼ˆä¿ç•™æœ€è¿‘çš„æ¡ç›®ï¼‰
        """
        try:
            all_rss_urls = self.rsshub_manager.get_all_rss_urls()
            max_known_items = 1000  # æ¯ä¸ªRSSæºæœ€å¤šä¿ç•™1000ä¸ªå·²çŸ¥æ¡ç›®

            for rss_url in all_rss_urls:
                try:
                    known_item_ids = self.rsshub_manager.get_known_item_ids(rss_url)
                    
                    if len(known_item_ids) > max_known_items:
                        # ä¿ç•™æœ€æ–°çš„æ¡ç›®ï¼ˆç®€å•çš„FIFOç­–ç•¥ï¼‰
                        trimmed_ids = known_item_ids[-max_known_items:]
                        self.rsshub_manager.save_known_item_ids(rss_url, trimmed_ids)
                        
                        removed_count = len(known_item_ids) - len(trimmed_ids)
                        self.logger.info(f"æ¸…ç†RSSæºè¿‡æœŸæ¡ç›®: {rss_url}, ç§»é™¤ {removed_count} ä¸ªæ—§æ¡ç›®")

                except Exception as e:
                    self.logger.warning(f"æ¸…ç†RSSæºå·²çŸ¥æ¡ç›®å¤±è´¥: {rss_url}, é”™è¯¯: {str(e)}")
                    continue

        except Exception as e:
            self.logger.error(f"æ¸…ç†å·²çŸ¥æ¡ç›®å¤±è´¥: {str(e)}", exc_info=True)

    def get_scheduler_stats(self) -> Dict[str, Any]:
        """
        è·å–è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # è·å–ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯
            manager_stats = self.rsshub_manager.get_statistics()

            # æ·»åŠ è°ƒåº¦å™¨ç‰¹å®šä¿¡æ¯
            scheduler_stats = {
                "check_interval": self.check_interval,
                "max_concurrent_feeds": self.max_concurrent_feeds,
                "max_items_per_batch": self.max_items_per_batch,
                "last_check_time": datetime.now().isoformat()
            }

            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            return {**manager_stats, **scheduler_stats}

        except Exception as e:
            self.logger.error(f"è·å–è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}", exc_info=True)
            return {}


# ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºRSSHubè°ƒåº¦å™¨å®ä¾‹
def create_rsshub_scheduler(data_dir: str = "data/rsshub") -> RSSHubScheduler:
    """
    åˆ›å»ºRSSHubè°ƒåº¦å™¨å®ä¾‹

    Args:
        data_dir: æ•°æ®å­˜å‚¨ç›®å½•

    Returns:
        RSSHubScheduler: RSSHubè°ƒåº¦å™¨å®ä¾‹
    """
    return RSSHubScheduler(data_dir)


if __name__ == "__main__":
    # æ¨¡å—æµ‹è¯•ä»£ç 
    import asyncio

    async def test_rsshub_scheduler():
        """æµ‹è¯•RSSHubè°ƒåº¦å™¨åŠŸèƒ½"""
        print("ğŸ§ª RSSHubè°ƒåº¦å™¨æ¨¡å—æµ‹è¯•")

        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = create_rsshub_scheduler("test_data/rsshub")
        print(f"âœ… åˆ›å»ºRSSHubè°ƒåº¦å™¨: {type(scheduler).__name__}")

        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = scheduler.get_scheduler_stats()
        print(f"âœ… è·å–ç»Ÿè®¡ä¿¡æ¯: {stats.get('total_rss_sources', 0)} ä¸ªRSSæº")

        # æµ‹è¯•æ•°æ®æ¸…ç†
        scheduler.cleanup_old_data()
        print("âœ… æ•°æ®æ¸…ç†ä»»åŠ¡å®Œæˆ")

        print("ğŸ‰ RSSHubè°ƒåº¦å™¨æ¨¡å—æµ‹è¯•å®Œæˆ")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_rsshub_scheduler()) 