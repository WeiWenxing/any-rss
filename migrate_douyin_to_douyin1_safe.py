#!/usr/bin/env python3
"""
Douyinåˆ°Douyin1å®‰å…¨æ•°æ®è¿ç§»è„šæœ¬

åŠŸèƒ½ï¼š
1. æ™ºèƒ½åˆå¹¶è®¢é˜…æ•°æ®ï¼ˆä¸è¦†ç›–ç°æœ‰è®¢é˜…ï¼‰
2. åˆå¹¶å·²çŸ¥å†…å®¹IDï¼ˆå»é‡å¤„ç†ï¼‰
3. åˆå¹¶æ¶ˆæ¯æ˜ å°„ï¼ˆä¿ç•™ç°æœ‰æ˜ å°„ï¼‰
4. å†²çªæ£€æµ‹å’Œå¤„ç†
5. å®Œæ•´çš„å¤‡ä»½å’Œå›æ»šæœºåˆ¶

ä½œè€…: Assistant
åˆ›å»ºæ—¶é—´: 2024å¹´
"""

import os
import json
import hashlib
import shutil
import logging
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from datetime import datetime


class SafeDouyinMigrator:
    """å®‰å…¨çš„Douyinæ•°æ®è¿ç§»å™¨"""
    
    def __init__(self, dry_run: bool = True):
        """
        åˆå§‹åŒ–å®‰å…¨è¿ç§»å™¨
        
        Args:
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼‰
        """
        self.dry_run = dry_run
        self.logger = self._setup_logger()
        
        # æºè·¯å¾„ï¼ˆdouyinï¼‰
        self.source_base = Path("storage/douyin")
        self.source_config = self.source_base / "config"
        self.source_data = self.source_base / "data"
        self.source_media = self.source_base / "media"
        
        # ç›®æ ‡è·¯å¾„ï¼ˆdouyin1ï¼‰
        self.target_base = Path("storage/douyin1")
        self.target_config = self.target_base / "config"
        self.target_data = self.target_base / "data"
        self.target_media = self.target_base / "media"
        
        # å¤‡ä»½è·¯å¾„
        self.backup_base = None
        
        # è¿ç§»ç»Ÿè®¡
        self.stats = {
            "subscriptions_merged": 0,
            "subscriptions_conflicts": 0,
            "known_items_merged": 0,
            "known_items_duplicates": 0,
            "message_mappings_merged": 0,
            "message_mappings_conflicts": 0,
            "directories_created": 0,
            "errors": [],
            "warnings": []
        }
        
        # URLåˆ°ç›®å½•çš„æ˜ å°„å…³ç³»
        self.url_directory_mapping = {}
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("safe_douyin_migrator")
        logger.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        log_file = f"safe_migration_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        
        return logger
    
    def _safe_filename(self, url: str) -> str:
        """
        ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶åï¼ˆå¤ç”¨douyin1çš„é€»è¾‘ï¼‰
        
        Args:
            url: URLå­—ç¬¦ä¸²
            
        Returns:
            str: å®‰å…¨çš„æ–‡ä»¶å
        """
        # ç§»é™¤åè®®å‰ç¼€
        clean_url = re.sub(r'^https?://', '', url)
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        clean_url = re.sub(r'[^\w\-_.]', '_', clean_url)
        # é™åˆ¶é•¿åº¦å¹¶æ·»åŠ å“ˆå¸Œ
        if len(clean_url) > 50:
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            clean_url = clean_url[:42] + '_' + url_hash
        
        return clean_url
    
    def _get_douyin_hash(self, url: str) -> str:
        """
        ç”Ÿæˆdouyinæ¨¡å—ä½¿ç”¨çš„å“ˆå¸Œå€¼
        
        Args:
            url: URLå­—ç¬¦ä¸²
            
        Returns:
            str: SHA256å“ˆå¸Œå€¼å‰16ä½
        """
        return hashlib.sha256(url.encode()).hexdigest()[:16]
    
    def _ensure_directories(self):
        """ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨"""
        directories = [
            self.target_base,
            self.target_config,
            self.target_data,
            self.target_media
        ]
        
        for directory in directories:
            if not self.dry_run:
                directory.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}ç¡®ä¿ç›®å½•å­˜åœ¨: {directory}")
            self.stats["directories_created"] += 1
    
    def _build_url_mapping(self) -> Dict[str, str]:
        """
        æ„å»ºURLåˆ°ç›®å½•çš„æ˜ å°„å…³ç³»
        
        Returns:
            Dict[str, str]: {URL: douyin_hash_dir}
        """
        mapping = {}
        
        if not self.source_data.exists():
            self.logger.warning(f"æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.source_data}")
            return mapping
        
        # éå†æ‰€æœ‰å“ˆå¸Œç›®å½•
        for hash_dir in self.source_data.iterdir():
            if not hash_dir.is_dir():
                continue
                
            url_file = hash_dir / "url.txt"
            if url_file.exists():
                try:
                    url = url_file.read_text(encoding='utf-8').strip()
                    mapping[url] = hash_dir.name
                    self.logger.debug(f"å‘ç°URLæ˜ å°„: {url} -> {hash_dir.name}")
                except Exception as e:
                    self.logger.error(f"è¯»å–URLæ–‡ä»¶å¤±è´¥: {url_file}, é”™è¯¯: {e}")
                    self.stats["errors"].append(f"è¯»å–URLæ–‡ä»¶å¤±è´¥: {url_file}")
        
        self.logger.info(f"æ„å»ºURLæ˜ å°„å®Œæˆï¼Œå…± {len(mapping)} ä¸ªURL")
        return mapping
    
    def _load_json_safe(self, file_path: Path) -> Dict:
        """
        å®‰å…¨åŠ è½½JSONæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: JSONæ•°æ®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–å‡ºé”™åˆ™è¿”å›ç©ºå­—å…¸
        """
        try:
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return {}
        except Exception as e:
            self.logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return {}
    
    def _save_json_safe(self, data: Dict, file_path: Path) -> bool:
        """
        å®‰å…¨ä¿å­˜JSONæ–‡ä»¶
        
        Args:
            data: è¦ä¿å­˜çš„æ•°æ®
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            if not self.dry_run:
                file_path.parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False
    
    def create_target_backup(self) -> bool:
        """
        åˆ›å»ºç›®æ ‡æ•°æ®å¤‡ä»½ï¼ˆdouyin1ç°æœ‰æ•°æ®ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            if not self.target_base.exists():
                self.logger.info("ç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€å¤‡ä»½")
                return True
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.backup_base = Path(f"storage/douyin1_backup_{timestamp}")
            
            if not self.dry_run:
                shutil.copytree(self.target_base, self.backup_base)
            
            self.logger.info(f"{'[DRY RUN] ' if self.dry_run else ''}åˆ›å»ºç›®æ ‡æ•°æ®å¤‡ä»½: {self.backup_base}")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºç›®æ ‡å¤‡ä»½å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"åˆ›å»ºç›®æ ‡å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def merge_subscriptions(self) -> bool:
        """
        æ™ºèƒ½åˆå¹¶è®¢é˜…æ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸ”„ å¼€å§‹åˆå¹¶è®¢é˜…æ•°æ®...")
            
            source_file = self.source_config / "subscriptions.json"
            target_file = self.target_config / "subscriptions.json"
            
            # åŠ è½½æºæ•°æ®å’Œç›®æ ‡æ•°æ®
            source_subs = self._load_json_safe(source_file)
            target_subs = self._load_json_safe(target_file)
            
            self.logger.info(f"æºè®¢é˜…æ•°æ®: {len(source_subs)} ä¸ªURL")
            self.logger.info(f"ç›®æ ‡è®¢é˜…æ•°æ®: {len(target_subs)} ä¸ªURL")
            
            # åˆå¹¶é€»è¾‘
            merged_subs = target_subs.copy()  # ä»ç°æœ‰æ•°æ®å¼€å§‹
            conflicts = []
            
            for url, source_channels in source_subs.items():
                if url in merged_subs:
                    # URLå·²å­˜åœ¨ï¼Œåˆå¹¶é¢‘é“åˆ—è¡¨
                    existing_channels = set(merged_subs[url])
                    new_channels = set(source_channels)
                    
                    # æ£€æŸ¥å†²çª
                    common_channels = existing_channels & new_channels
                    if common_channels:
                        conflicts.append({
                            "url": url,
                            "common_channels": list(common_channels),
                            "source_channels": source_channels,
                            "target_channels": merged_subs[url]
                        })
                        self.logger.warning(f"âš ï¸ è®¢é˜…å†²çª: {url} åœ¨ä¸¤ä¸ªæ¨¡å—ä¸­éƒ½æœ‰ç›¸åŒé¢‘é“: {list(common_channels)}")
                    
                    # åˆå¹¶é¢‘é“ï¼ˆå»é‡ï¼‰
                    all_channels = existing_channels | new_channels
                    merged_subs[url] = list(all_channels)
                    
                    self.logger.info(f"ğŸ”— åˆå¹¶è®¢é˜…: {url} ({len(existing_channels)} + {len(new_channels)} -> {len(all_channels)} ä¸ªé¢‘é“)")
                else:
                    # æ–°URLï¼Œç›´æ¥æ·»åŠ 
                    merged_subs[url] = source_channels
                    self.logger.info(f"â• æ–°å¢è®¢é˜…: {url} ({len(source_channels)} ä¸ªé¢‘é“)")
            
            # ä¿å­˜åˆå¹¶ç»“æœ
            if not self._save_json_safe(merged_subs, target_file):
                return False
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["subscriptions_merged"] = len(source_subs)
            self.stats["subscriptions_conflicts"] = len(conflicts)
            
            # è®°å½•å†²çªè¯¦æƒ…
            if conflicts:
                self.logger.warning(f"âš ï¸ å‘ç° {len(conflicts)} ä¸ªè®¢é˜…å†²çªï¼Œè¯¦æƒ…ï¼š")
                for conflict in conflicts:
                    self.logger.warning(f"  URL: {conflict['url']}")
                    self.logger.warning(f"  å†²çªé¢‘é“: {conflict['common_channels']}")
                    self.stats["warnings"].append(f"è®¢é˜…å†²çª: {conflict['url']} - {conflict['common_channels']}")
            
            self.logger.info(f"âœ… è®¢é˜…æ•°æ®åˆå¹¶å®Œæˆ: æ€»è®¡ {len(merged_subs)} ä¸ªURL")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆå¹¶è®¢é˜…æ•°æ®å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"åˆå¹¶è®¢é˜…æ•°æ®å¤±è´¥: {e}")
            return False
    
    def merge_known_items(self) -> bool:
        """
        æ™ºèƒ½åˆå¹¶å·²çŸ¥å†…å®¹IDæ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸ”„ å¼€å§‹åˆå¹¶å·²çŸ¥å†…å®¹æ•°æ®...")
            
            merged_count = 0
            duplicate_count = 0
            
            for url, hash_dir in self.url_directory_mapping.items():
                source_dir = self.source_data / hash_dir
                source_file = source_dir / "known_item_ids.json"
                
                if not source_file.exists():
                    continue
                
                # åŠ è½½æºæ•°æ®
                source_items = self._load_json_safe(source_file)
                if not isinstance(source_items, list):
                    self.logger.warning(f"æºå·²çŸ¥å†…å®¹æ•°æ®æ ¼å¼é”™è¯¯: {source_file}")
                    continue
                
                # ç¡®å®šç›®æ ‡æ–‡ä»¶è·¯å¾„
                safe_name = self._safe_filename(url)
                target_dir = self.target_data / safe_name
                target_file = target_dir / "known_item_ids.json"
                
                # åŠ è½½ç›®æ ‡æ•°æ®
                target_items = self._load_json_safe(target_file)
                if not isinstance(target_items, list):
                    target_items = []
                
                # åˆå¹¶å·²çŸ¥å†…å®¹ï¼ˆå»é‡ï¼‰
                source_set = set(source_items)
                target_set = set(target_items)
                
                duplicates = source_set & target_set
                if duplicates:
                    self.logger.info(f"ğŸ” å‘ç°é‡å¤å†…å®¹: {url} ({len(duplicates)} ä¸ª)")
                    duplicate_count += len(duplicates)
                
                # åˆå¹¶å¹¶ä¿æŒåŸæœ‰é¡ºåº
                merged_items = target_items.copy()
                for item in source_items:
                    if item not in target_set:
                        merged_items.append(item)
                
                # ä¿å­˜åˆå¹¶ç»“æœ
                if not self.dry_run:
                    target_dir.mkdir(parents=True, exist_ok=True)
                
                if not self._save_json_safe(merged_items, target_file):
                    continue
                
                merged_count += 1
                self.logger.info(f"ğŸ”— åˆå¹¶å·²çŸ¥å†…å®¹: {url} ({len(target_items)} + {len(source_items)} -> {len(merged_items)} ä¸ª)")
            
            self.stats["known_items_merged"] = merged_count
            self.stats["known_items_duplicates"] = duplicate_count
            
            self.logger.info(f"âœ… å·²çŸ¥å†…å®¹åˆå¹¶å®Œæˆ: {merged_count} ä¸ªURLï¼Œ{duplicate_count} ä¸ªé‡å¤é¡¹")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆå¹¶å·²çŸ¥å†…å®¹å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"åˆå¹¶å·²çŸ¥å†…å®¹å¤±è´¥: {e}")
            return False
    
    def merge_message_mappings(self) -> bool:
        """
        æ™ºèƒ½åˆå¹¶æ¶ˆæ¯æ˜ å°„æ•°æ®
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸ”„ å¼€å§‹åˆå¹¶æ¶ˆæ¯æ˜ å°„æ•°æ®...")
            
            source_file = self.source_config / "message_mappings.json"
            target_file = self.target_config / "message_mappings.json"
            
            # åŠ è½½æºæ•°æ®å’Œç›®æ ‡æ•°æ®
            source_mappings = self._load_json_safe(source_file)
            target_mappings = self._load_json_safe(target_file)
            
            self.logger.info(f"æºæ¶ˆæ¯æ˜ å°„: {len(source_mappings)} ä¸ªURL")
            self.logger.info(f"ç›®æ ‡æ¶ˆæ¯æ˜ å°„: {len(target_mappings)} ä¸ªURL")
            
            # åˆå¹¶é€»è¾‘
            merged_mappings = target_mappings.copy()  # ä»ç°æœ‰æ•°æ®å¼€å§‹
            conflicts = []
            
            for url, source_url_mappings in source_mappings.items():
                if url in merged_mappings:
                    # URLå·²å­˜åœ¨ï¼Œåˆå¹¶itemæ˜ å°„
                    existing_mappings = merged_mappings[url]
                    
                    for item_id, source_item_mappings in source_url_mappings.items():
                        if item_id in existing_mappings:
                            # item_idå·²å­˜åœ¨ï¼Œåˆå¹¶é¢‘é“æ˜ å°„
                            existing_item_mappings = existing_mappings[item_id]
                            
                            for chat_id, source_msg_ids in source_item_mappings.items():
                                if chat_id in existing_item_mappings:
                                    # é¢‘é“å·²å­˜åœ¨ï¼Œæ£€æŸ¥å†²çª
                                    existing_msg_ids = existing_item_mappings[chat_id]
                                    if existing_msg_ids != source_msg_ids:
                                        conflicts.append({
                                            "url": url,
                                            "item_id": item_id,
                                            "chat_id": chat_id,
                                            "source_msg_ids": source_msg_ids,
                                            "target_msg_ids": existing_msg_ids
                                        })
                                        self.logger.warning(f"âš ï¸ æ¶ˆæ¯æ˜ å°„å†²çª: {url}/{item_id}/{chat_id}")
                                        # ä¿æŒç°æœ‰æ˜ å°„ï¼Œä¸è¦†ç›–
                                    else:
                                        self.logger.debug(f"æ¶ˆæ¯æ˜ å°„ä¸€è‡´: {url}/{item_id}/{chat_id}")
                                else:
                                    # æ–°é¢‘é“ï¼Œç›´æ¥æ·»åŠ 
                                    existing_item_mappings[chat_id] = source_msg_ids
                        else:
                            # æ–°item_idï¼Œç›´æ¥æ·»åŠ 
                            existing_mappings[item_id] = source_item_mappings
                else:
                    # æ–°URLï¼Œç›´æ¥æ·»åŠ 
                    merged_mappings[url] = source_url_mappings
                    self.logger.info(f"â• æ–°å¢æ¶ˆæ¯æ˜ å°„: {url}")
            
            # ä¿å­˜åˆå¹¶ç»“æœ
            if not self._save_json_safe(merged_mappings, target_file):
                return False
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["message_mappings_merged"] = len(source_mappings)
            self.stats["message_mappings_conflicts"] = len(conflicts)
            
            # è®°å½•å†²çªè¯¦æƒ…
            if conflicts:
                self.logger.warning(f"âš ï¸ å‘ç° {len(conflicts)} ä¸ªæ¶ˆæ¯æ˜ å°„å†²çªï¼Œä¿æŒç°æœ‰æ˜ å°„")
                for conflict in conflicts:
                    self.logger.warning(f"  {conflict['url']}/{conflict['item_id']}/{conflict['chat_id']}")
                    self.stats["warnings"].append(f"æ¶ˆæ¯æ˜ å°„å†²çª: {conflict['url']}/{conflict['item_id']}/{conflict['chat_id']}")
            
            self.logger.info(f"âœ… æ¶ˆæ¯æ˜ å°„åˆå¹¶å®Œæˆ: æ€»è®¡ {len(merged_mappings)} ä¸ªURL")
            return True
            
        except Exception as e:
            self.logger.error(f"åˆå¹¶æ¶ˆæ¯æ˜ å°„å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"].append(f"åˆå¹¶æ¶ˆæ¯æ˜ å°„å¤±è´¥: {e}")
            return False
    
    def detect_conflicts(self) -> Dict[str, List]:
        """
        æ£€æµ‹æ½œåœ¨å†²çª
        
        Returns:
            Dict[str, List]: å†²çªæŠ¥å‘Š
        """
        conflicts = {
            "subscription_conflicts": [],
            "directory_conflicts": [],
            "url_conflicts": []
        }
        
        try:
            self.logger.info("ğŸ” æ£€æµ‹æ½œåœ¨å†²çª...")
            
            # æ£€æŸ¥è®¢é˜…å†²çª
            source_subs = self._load_json_safe(self.source_config / "subscriptions.json")
            target_subs = self._load_json_safe(self.target_config / "subscriptions.json")
            
            for url in source_subs:
                if url in target_subs:
                    source_channels = set(source_subs[url])
                    target_channels = set(target_subs[url])
                    common_channels = source_channels & target_channels
                    
                    if common_channels:
                        conflicts["subscription_conflicts"].append({
                            "url": url,
                            "common_channels": list(common_channels),
                            "source_only": list(source_channels - target_channels),
                            "target_only": list(target_channels - source_channels)
                        })
            
            # æ£€æŸ¥ç›®å½•å†²çª
            for url in self.url_directory_mapping:
                safe_name = self._safe_filename(url)
                target_dir = self.target_data / safe_name
                
                if target_dir.exists():
                    conflicts["directory_conflicts"].append({
                        "url": url,
                        "safe_name": safe_name,
                        "target_dir": str(target_dir)
                    })
            
            # æ±‡æ€»æŠ¥å‘Š
            total_conflicts = (len(conflicts["subscription_conflicts"]) + 
                             len(conflicts["directory_conflicts"]) + 
                             len(conflicts["url_conflicts"]))
            
            if total_conflicts > 0:
                self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ° {total_conflicts} ä¸ªæ½œåœ¨å†²çª")
                self.logger.warning(f"  - è®¢é˜…å†²çª: {len(conflicts['subscription_conflicts'])} ä¸ª")
                self.logger.warning(f"  - ç›®å½•å†²çª: {len(conflicts['directory_conflicts'])} ä¸ª")
                self.logger.warning(f"  - URLå†²çª: {len(conflicts['url_conflicts'])} ä¸ª")
            else:
                self.logger.info("âœ… æœªæ£€æµ‹åˆ°å†²çª")
            
            return conflicts
            
        except Exception as e:
            self.logger.error(f"æ£€æµ‹å†²çªå¤±è´¥: {e}", exc_info=True)
            return conflicts
    
    def rollback_changes(self) -> bool:
        """
        å›æ»šæ›´æ”¹ï¼ˆæ¢å¤å¤‡ä»½ï¼‰
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            if not self.backup_base or not self.backup_base.exists():
                self.logger.error("æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ•°æ®ï¼Œæ— æ³•å›æ»š")
                return False
            
            self.logger.info(f"ğŸ”„ å¼€å§‹å›æ»šæ›´æ”¹ï¼Œæ¢å¤å¤‡ä»½: {self.backup_base}")
            
            # åˆ é™¤å½“å‰ç›®æ ‡ç›®å½•
            if self.target_base.exists():
                shutil.rmtree(self.target_base)
            
            # æ¢å¤å¤‡ä»½
            shutil.copytree(self.backup_base, self.target_base)
            
            self.logger.info("âœ… å›æ»šå®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"å›æ»šå¤±è´¥: {e}", exc_info=True)
            return False
    
    def run_safe_migration(self, create_backup: bool = True) -> bool:
        """
        æ‰§è¡Œå®‰å…¨è¿ç§»æµç¨‹
        
        Args:
            create_backup: æ˜¯å¦åˆ›å»ºå¤‡ä»½
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        self.logger.info(f"ğŸ›¡ï¸ å¼€å§‹å®‰å…¨æ•°æ®è¿ç§» {'(è¯•è¿è¡Œæ¨¡å¼)' if self.dry_run else '(å®é™…æ‰§è¡Œ)'}")
        
        try:
            # 1. åˆ›å»ºç›®æ ‡æ•°æ®å¤‡ä»½
            if create_backup:
                if not self.create_target_backup():
                    return False
            
            # 2. ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            self._ensure_directories()
            
            # 3. æ„å»ºURLæ˜ å°„
            self.url_directory_mapping = self._build_url_mapping()
            
            # 4. æ£€æµ‹å†²çª
            conflicts = self.detect_conflicts()
            
            # 5. åˆå¹¶è®¢é˜…æ•°æ®
            if not self.merge_subscriptions():
                return False
            
            # 6. åˆå¹¶å·²çŸ¥å†…å®¹
            if not self.merge_known_items():
                return False
            
            # 7. åˆå¹¶æ¶ˆæ¯æ˜ å°„
            if not self.merge_message_mappings():
                return False
            
            # 8. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            self.print_statistics()
            
            self.logger.info("ğŸ‰ å®‰å…¨æ•°æ®è¿ç§»å®Œæˆï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"å®‰å…¨è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            
            # å°è¯•å›æ»š
            if not self.dry_run and self.backup_base:
                self.logger.info("ğŸ”„ å°è¯•å›æ»šæ›´æ”¹...")
                self.rollback_changes()
            
            return False
    
    def print_statistics(self):
        """æ‰“å°è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š å®‰å…¨è¿ç§»ç»Ÿè®¡ä¿¡æ¯")
        self.logger.info("=" * 60)
        self.logger.info(f"è®¢é˜…æ•°æ®åˆå¹¶: {self.stats['subscriptions_merged']} ä¸ª")
        self.logger.info(f"è®¢é˜…å†²çªå¤„ç†: {self.stats['subscriptions_conflicts']} ä¸ª")
        self.logger.info(f"å·²çŸ¥å†…å®¹åˆå¹¶: {self.stats['known_items_merged']} ä¸ªURL")
        self.logger.info(f"é‡å¤å†…å®¹å»é™¤: {self.stats['known_items_duplicates']} ä¸ª")
        self.logger.info(f"æ¶ˆæ¯æ˜ å°„åˆå¹¶: {self.stats['message_mappings_merged']} ä¸ª")
        self.logger.info(f"æ¶ˆæ¯æ˜ å°„å†²çª: {self.stats['message_mappings_conflicts']} ä¸ª")
        self.logger.info(f"åˆ›å»ºç›®å½•æ•°é‡: {self.stats['directories_created']} ä¸ª")
        self.logger.info(f"é”™è¯¯æ•°é‡: {len(self.stats['errors'])} ä¸ª")
        self.logger.info(f"è­¦å‘Šæ•°é‡: {len(self.stats['warnings'])} ä¸ª")
        
        if self.stats['errors']:
            self.logger.info("\nâŒ é”™è¯¯è¯¦æƒ…:")
            for error in self.stats['errors']:
                self.logger.info(f"  - {error}")
        
        if self.stats['warnings']:
            self.logger.info("\nâš ï¸ è­¦å‘Šè¯¦æƒ…:")
            for warning in self.stats['warnings']:
                self.logger.info(f"  - {warning}")
        
        self.logger.info("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Douyinåˆ°Douyin1å®‰å…¨æ•°æ®è¿ç§»å·¥å…·')
    parser.add_argument('--dry-run', action='store_true', 
                       help='è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶')
    parser.add_argument('--no-backup', action='store_true',
                       help='ä¸åˆ›å»ºå¤‡ä»½')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--rollback', type=str, metavar='BACKUP_DIR',
                       help='å›æ»šåˆ°æŒ‡å®šå¤‡ä»½ç›®å½•')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # å¤„ç†å›æ»šè¯·æ±‚
    if args.rollback:
        backup_path = Path(args.rollback)
        if not backup_path.exists():
            print(f"âŒ å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: {backup_path}")
            exit(1)
        
        try:
            target_path = Path("storage/douyin1")
            if target_path.exists():
                shutil.rmtree(target_path)
            shutil.copytree(backup_path, target_path)
            print(f"âœ… å›æ»šå®Œæˆ: {backup_path} -> {target_path}")
            exit(0)
        except Exception as e:
            print(f"âŒ å›æ»šå¤±è´¥: {e}")
            exit(1)
    
    # åˆ›å»ºå®‰å…¨è¿ç§»å™¨
    migrator = SafeDouyinMigrator(dry_run=args.dry_run)
    
    # æ‰§è¡Œå®‰å…¨è¿ç§»
    success = migrator.run_safe_migration(create_backup=not args.no_backup)
    
    if success:
        print("âœ… å®‰å…¨è¿ç§»å®Œæˆï¼")
        if args.dry_run:
            print("ğŸ’¡ è¿™æ˜¯è¯•è¿è¡Œæ¨¡å¼ï¼Œè¦å®é™…æ‰§è¡Œè¯·ç§»é™¤ --dry-run å‚æ•°")
        else:
            print("ğŸ’¾ å·²åˆ›å»ºå¤‡ä»½ï¼Œå¦‚éœ€å›æ»šè¯·ä½¿ç”¨ --rollback å‚æ•°")
    else:
        print("âŒ å®‰å…¨è¿ç§»å¤±è´¥ï¼è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶")
        exit(1)


if __name__ == "__main__":
    main() 